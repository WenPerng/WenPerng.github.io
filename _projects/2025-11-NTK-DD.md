---
title: "Neural Tangent Kernel: on Double Descent"
collection: projects
category: school
permalink: /projects/2025-11-NTK-DD
# venue: "Department of Electrical Engineering"
date: 2025-11-12
# location: "National Taiwan University"
excerpt: "This presentation discusses the double descent behavior of neural network training by linking it with that of kernel regression via the Neural Tangent Kernel. Along the way, the replica trick is introduced to derive the analytic formula for the generalization error of kernel regression. *Course presentation for the course \"Physical Theories of (Machine) Learning\" given in the fall of 2025 by Prof. [Miranda Chih-Ning Cheng](https://sites.google.com/site/mcheng0606/).*"
---

> This is a course presentation for the course \"Physical Theories of (Machine) Learning\" given in the fall of 2025 by Prof. [Miranda Chih-Ning Cheng](https://sites.google.com/site/mcheng0606/)

# Abstract
The classical bias-variance tradeoff predicts a U-shaped curve for generalization error: increasing model complexity initially reduces error, but eventually leads to overfitting at the interpolation threshold. Modern machine learning, however, exhibits a *double-descent* behavior in which the generalization error decreases again as model size grows far beyond this threshold. This phenomenon suggests that overparameterizaiton can, counterintuitively, improve performance.

In this presentation, I relate the double-descent behavior in neural network training to that of ridgeless (\\(\lambda \rightarrow 0\\)) kernel regression via the Neural Tangent Kernel. I incorporate basics from random matrix theory to clarify the underlying mechanisms. I then introduce the replica trick to derive an analytic expression for the generalization error in kernel regression, providing theoretical insight into when and why double descent arises.
# Slides
Download the slides for the talk [here](/files/slides/projects/2025_NTK_Double_Descent.pdf).

<embed src="/files/slides/projects/2025_NTK_Double_Descent.pdf" type="application/pdf" width="100%" height="600px" />


**Enjoy the slides above?** You can use the template and modify it to your own use! The \\(\LaTeX\\) source code to the slides can be found [here](https://github.com/WenPerng/NTUEE_Projects/tree/main/Physical_Theory_of_Machine_Learning/NTK_Double_Descent_slides).

The MATLAB code used to generate the figure below can be found [here](https://github.com/WenPerng/NTUEE_Projects/blob/main/Physical_Theory_of_Machine_Learning/DoubleDescent.m).

<img src='/images/projects/2025_doubleDescent.png' width="80%">

# Main References
- Adlam et al., "[The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization](https://arxiv.org/abs/2008.06786)."
- Canatar et al., "[Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks](https://www.nature.com/articles/s41467-021-23103-1)."
- R. Speicher, "[High Dimensional Analysis: Random Matrices and Machine Learning](https://rolandspeicher.com/lectures/course-on-high-dimensional-analysis-random-matrices-and-machine-learning-summer-term-2023/)," lecture notes, Saarland University.