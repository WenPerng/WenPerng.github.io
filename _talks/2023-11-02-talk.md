---
title: "Matrix Differentiation"
collection: talks
type: "Talk"
permalink: /talks/2023-11-02-talk
venue: "Department of Electrical Engineering, National Taiwan University"
date: 2023-11-02
location: "Taipei, Taiwan"
---

What is matrix differentiation? How do we make sense of the notation \\(\frac{\mathrm{d} f}{\mathrm{d} A}\\), where \\(f\\) is a scalar function of a matrix \\(A\\)? This talk answers just that.

[Slides](https://github.com/WenPerng/EESAAD_slides/blob/8bb7e14a4bd44e8dd70803546ddef0b3ab2adf02/Matrix%20Differentiation%202023%20%5Bwritten%5D.pdf) | 
[Recordings](https://www.youtube.com/watch?v=YRVq5iREDJo)

Starting from the notion of Fr√©chet derivative, we motivate treating the differential as a linear operator between vector spaces. This allows us to easily derive the first and second derivative of a scalar function on vectors (the gradient \\(\nabla f\\) and Hessian \\(\nabla\nabla^{\mathsf{T}}f\\)). We further demonstrate what differentiation of a scalar function by a matrix is.

Along the way, the infamous notational jargon of "numerator layout" and "denominator layout" of matrix differentiation is also clarified.

Lastly, we go over some examples and applications of matrix differentiation: this includes the derivative of an inverse matrix, the linear least mean square estimator, the derivative of eigenvalues, and etc.
