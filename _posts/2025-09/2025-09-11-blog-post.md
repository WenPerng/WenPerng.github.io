---
title: 'Steepest Descent on Normed Manifolds'
date: 2025-09-11
permalink: /posts/2025/09/blog-post-11/
tags:
  - manifold optimization
  - optimization
  - geometry
excerpt: "This post clarifies what steepest descent on a normed manifold means."
---

Continue from [last post](/posts/2025/09/blog-post-10/) where I discussed about Riemannian manifold optimizaiton, I extend my discussions on [steepest descent on normed vector spaces](/posts/2025/09/blog-post-07/) to normed manifolds.

I write this post after reading the works from [this site](https://docs.modula.systems/algorithms/manifold/). I do not like their formulation and introduction there. Hence, I decide to derive similar results from ground up in my own words.


From last post, I have established that to minimize a function defined on a manifold \\(f:\mathcal{M} \rightarrow \mathbb{R}\\), we apply the following two steps:
<p>

$$
{\begin{aligned}
    h_t &= \arg\min_{h\in T_{x_{t}}\mathcal{M}} \left\{ \frac{\partial f(x_{t})}{\partial x}[h] + \frac{\lambda}{2} \langle h,h \rangle _{x_{t}}\right\}, \\
    x_{t+1} &= R_{x_t}(h_t).
\end{aligned}}
$$
</p>
The function \\(R\\) is a retraction chosen.

# Steepest descent on normed manifolds
The [post](https://leloykun.github.io/ponder/steepest-descent-non-riemannian/) by Franz Louis Cesista explained really well on this topic. My post is heavily influenced by it.

Consider on the tangent bundle \\(T\mathcal{M} = \cup_{x\in\mathcal{M}} T_x\mathcal{M}\\), instead of having an inner product \\(\langle\cdot,\cdot\rangle_x\\) associated to each tangent space \\(T_x\mathcal{M}\\), we have a norm \\(\|\|\cdot\|\|_x\\) on it. If the norm is induced by an inner product, we are doing Riemannian optimization; else, we are doing non-Riemannian optimization.

The descent direction \\(h\\) will then be
<p>

$$
\boxed{
    h = \arg\min_{h\in T_{x_{t}}\mathcal{M}} \left\{ \frac{\partial f(x_{t})}{\partial x}[h] + \frac{\lambda}{2} \|h\|_x^2 \right\}.
}
$$
</p>
Solving this optimization then applying the retraction map allows us to do steepest descent algorithms on normed manifolds.

Another name for a normed manifold is *a manifold with Finsler structure*, or simply, a *Finsler manifold*. This is different from the usual Riemannian manifold that I had dealt with before.

## The descent direction
Furthermore, by adopting the dualization operation (I have discussed this in [a previous post](https://wenperng.github.io/posts/2025/09/blog-post-07/)) which is from the paper "[Modular Duality in Deep Learning](https://arxiv.org/pdf/2410.21265#:~:text=Modular%20dualization%20involves%20first%20assigning%20operator%20norms%20to,the%20weight%20space%20of%20the%20full%20neural%20architecture.)", we have the solution to the descent direction as

<p>

$$
h = -\frac{1}{\lambda} \left\|\frac{\partial f(x_t)}{\partial x}\right\|_* \cdot \mathrm{dualize}_{\|\cdot\|}\left(\frac{\partial f(x_t)}{\partial x}\right).
$$
</p>

If one uses the more *differential-geometry*-inclined notation of the differential map, where \\(\partial f/\partial x = \mathrm{D}f = \mathrm{d}f = f_\*\\) is equivalent to the pushforward map, then we can also write the descent direction simply as
<p>

$$\boxed{
h = -\frac{\|f_*(x_t)\|_*}{\lambda} \cdot \mathrm{dualize}_{\|\cdot\|}(f_*(x_t)).
}$$
</p>
Note that the pushforward map \\(f _\*(x _{t}) : T _{x _t} \mathcal{M} \rightarrow \mathbb{R}\\). Hence, \\(f _\*(x _{t}) \in T _{x _t} ^\* \mathcal{M}\\) is a dual vector.

The steepest descent update can then be written as
<p>

$$
\boxed{\begin{aligned}
    x_{t+1} &= R_{x_t}\left( - \eta \|f_*(x_t)\|_* \cdot \mathrm{dualize}_{\|\cdot\|}(f_*(x_t))\right) \\
    &= R_{x_t}\left( - \frac{1}{\lambda} \|f_*(x_t)\|_* \cdot \mathrm{dualize}_{\|\cdot\|}(f_*(x_t))\right).
\end{aligned}}
$$
</p>
The sharpness parameter and step-size are related by \\(\lambda = 1/\eta\\).

# Why normed?
Here I give another example as to when we will meet an optimization problem over the Finsler manifold as opposed to treating the formulation above merely as a play of symbols.

Suppose our loss function \\(f\\) can be locally bounded by the quadratic function
<p>

$$
f(R_x(h)) \le f(x) + \frac{\partial f(x)}{\partial x}[h] + \frac{\lambda}{2} \|h\|^2.
$$
</p>
Then a simple way to upper bound \\(f\\) is to find the minima of the right hand side, which results in the minimization problem
<p>

$$
\begin{aligned}
    h &= \arg\min_{h\in T_x\mathcal{M}} \left\{f(x) + \frac{\partial f(x)}{\partial x}[h] + \frac{\lambda}{2} \|h\|^2\right\} \\
    &= \arg\min_{h\in T_x\mathcal{M}} \left\{\frac{\partial f(x)}{\partial x}[h] + \frac{\lambda}{2} \|h\|^2\right\}.
\end{aligned}
$$
</p>
The minimization scheme proposed at the start of the post naturally appears.

# Notes on notation
Note that if \\(\mathcal{M}\\) is a matrix manifold, then we can rewrite

<p>

$$
\frac{\partial f(x)}{\partial x}[h] = \mathrm{Tr}\{g^{\mathsf{T}}h\} = \langle g,h\rangle _F,
$$
</p>
where \\(g\\) is some *gradient-like* matrix, and \\(\langle\cdot,\cdot\rangle_F\\) is the Frobenius inner product.

Such form must exist as the pushforward is a linear map \\(T_x\mathcal{M}\rightarrow \mathbb{R}\\), and any linear map mapping matrices to scalars can always be written as a Frobenius inner product.