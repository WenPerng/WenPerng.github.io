---
title: 'Newton-Schulz Iteration'
date: 2025-09-08
permalink: /posts/2025/09/blog-post-08/
tags:
  - optimization
  - linear algebra
excerpt: ""
---

> This post talks about the Newton-Schulz iteration method which uses only matrix multiplications (no inverses or other decomposition method) to perform a polynomial transform on the singular values of a matrix. This induces a fast iteration method for inverting or orthogonalizing a matrix with only matrix multiplication.

# Polynomial transform on singular values
Consider a matrix \\(X\\) with the singular value transform \\(U\Sigma V^\mathsf{H}\\). Observe the polynomial relation below:
<p>

$$
X (X^\mathsf{H}X)^n = U\Sigma V^\mathsf{H} \left(V \Sigma^\mathsf{T}\Sigma V^\mathsf{H}\right)^n = U f_{x^{2n+1}}(\Sigma) V^{\mathsf{H}},
$$
</p>
where \\(f_{x^{2n+1}}(\Sigma)\\) performs the function \\(x^{2n+1}\\) on the singular values on the diagonal of \\(\Sigma\\).

Using this idea, we can perform polynomials of odd orders \\(f(x) = a_0 x + a_1 x^3 + a_2 x^5 + \cdots\\) on any matrix by the polynomial transform
<p>

$$
f(X) = U f(\Sigma) V^\mathsf{H} = \sum_{n=0}^{\infty} a_n X (X^\mathsf{H} X)^n.
$$
</p>

# Orthogonalizing a matrix
Let us define what I mean by orthogonalizing a matrix. I want to find the solution to
<p>

$$
\arg\min_{M} \|M-X\|_F \text{ s.t. } MM^\mathsf{H} = \mathbb{I} \text{ or } M^\mathsf{H}M = \mathbb{I}.
$$
</p>
The solution to this optimization problem is simple, one can *easily* see that the optimal \\(M\\) is
<p>

$$
M = UV^\mathsf{H} \text{ where } X = U\Sigma V^\mathsf{H}.
$$
</p>
I.e., the orthogonalization is equal to applying the function \\(f(x) = 1\\) on the matrix.

However, with finite terms of the series applied, this can never be achieved. Instead, we consider a function \\(g\\) that is a polynomial with finite terms, and perform the composition \\(g\circ g\circ \cdots\circ g = g^{\circ n}\\) and have \\(g^{\circ n }\xrightarrow{n\rightarrow\infty} f\\) for a small range of inputs.

## Example
For example, let us consider a third order transform
<p>

$$
M_{t+1} = 2 M_{t} - 1.5 M_t(M_t^\mathsf{T}M_t) + 0.5 M_t(M_t^\mathsf{T}M_t)^2
$$
</p>
with \\(M_0 = X\\). This is equivalent to applying the polynomial \\(g(x) = 2x - 1.5x^3 + 0.5x^5\\) on the singular values. By iterating \\(g\\) (see the figure below), we see that it maps all singular values in \\((0,\sqrt{2})\\).

<img src='/images/posts/2025-09-08-orthogonalization.png'>

# Inverting a matrix
To invert a matrix \\(A\\), we perform the function \\(f(x) = 1/x\\) on the singular values. Furthermore, the left and right singular-vector matrices have to be Hermitian conjugated as well. The polynomial transform will hence be modified to be
<p>

$$
f(A) = V f(\Sigma) U^\mathsf{H} = \sum_{n=0}^{\infty} a_n A^\mathsf{H}(AA^\mathsf{H})^n.
$$
</p>

How do we realize the function \\(f\\) with finite terms? We again can consider an function \\(g\\) that gives us a good enough approximation to \\(f\\) as \\(g^{\circ n} \xrightarrow{n\rightarrow\infty} f\\).

Note that, however,
<p>

$$
f(x) = \frac{1}{1-(1-x)} = 1 + (1-x) + (1-x)^2 + \cdots,
$$
</p>
which is not an odd polynomial. What can we do then? By replacing the \\(A^\mathsf{H}\\) above with a matrix \\(X\\) that represents \\(A^{-1}\\), the function becomes a polynomial in \\(X\\) with both even and odd powers!

Let \\(X_0\\) be a well-selected initial condition, with \\(X_{t+1} = g_k(X_{t})\\) defined as
<p>

$$
X_{t+1} = X_{t} \sum_{n=0}^{k} (\mathbb{I} - AX_{t})^n = g_k(X_t).
$$
</p>
Note that as \\(k\rightarrow\infty\\), we obtain that \\(X_{t+1} = A^{-1}\\).

The iteration we obtained is the Newton-Schulz iteration for finding matrix inverses!

If we let \\(k=1\\), then we have
<p>

$$
X_{t+1} = X_{t} \left(2\mathbb{I} - AX_{t}\right).
$$
</p>
This is the most common version of Newton-Schulz iteration. If we let \\(k=2\\), then we have
<p>

$$
X_{t+1} = X_{t} \left(3\mathbb{I} - 3AX_{t} + (A X_{t})^2\right).
$$
</p>

The stability to the Newton-Schulz iteration for finding matrix inverses needs further examination. I might talk about this in a future post.