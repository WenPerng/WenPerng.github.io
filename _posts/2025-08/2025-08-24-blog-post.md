---
title: 'Paper review: Geodesics of Stiefel Manifold (I)'
date: 2025-08-24
permalink: /posts/2025/08/blog-post-4/
tags:
  - manifold optimization
  - geometry
excerpt: "This post is a review on the paper \"[The Geoemtry of Algorithms with Orthogonality Constraints](https://arxiv.org/abs/physics/9806030)\" by Edelman et al. The focus of this review is on deriving the geodesics on the Stiefel manifold with respect to the Frobenius inner product."
---

This post is a review on the paper "[The Geoemtry of Algorithms with Orthogonality Constraints](https://arxiv.org/abs/physics/9806030)" by Edelman et al. This is a lengthy paper. The goal of this review is not to go through the entire paper, but to focus on deriving the explicit form of geodesics on the Stiefel manifold with respect to the Frobenius inner product. The other interesting aspects to this paper is shared in future posts.

I have always wanted to know how the geodesics equation to the Stiefel manifold is obtained since I took the course "Manifold Optimization for Representation Learning" from professor [Hao Shen](https://www.researchgate.net/profile/Hao-Shen-15) from the Technische Universität München, 2024.

Thanks to a friend of mine (郭庭愷) who introduced me to this paper, I am finally able to understand the details.

# Basics on the Geometry of the Stiefel Manifold
Most of this section is established in my talk on [manifold optimization, 2024](/talks/2024-10-01-talk). If you are not sure of the details, be sure to check the slides.

<br><img src='/images/posts/2025-08-24-stiefel-geodesic.png'>

## Definition
The Stiefel manifold is a generalization to the orthogonal matrices, defined as
<p>
$$
\boxed{\begin{equation*}
	\mathrm{St}(m,k) = \left\{ X \in \mathbb{R}^{m \times k} \middle\vert X^{\mathsf{T}} X = \mathbb{I}_k\right\}
\end{equation*}},
$$
</p>
where \\(m\ge k\\) and \\(\mathbb{I}_k\\) is the \\(k \times k\\) identity matrix. Its dimension is
<p>
$$
	\dim\left( \mathrm{St}(m,k) \right) = mk - \frac{k(k+1)}{2}.
$$
</p>
This is easily obtained from the regular value level set theorem[^1].

[^1]: See: Tu, Loring W., "An introduction to manifolds." (2007).

## Tangent space
The tangent space to the Stiefel manifold at \\(X \in \mathrm{St}(m,k)\\) is
<p>
$$
\boxed{\begin{equation}\begin{aligned}
	T_{X} \mathrm{St}(m,k) &= \left\{ H \in \mathbb{R}^{m \times k} \middle\vert H^{\mathsf{T}} X + X^{\mathsf{T}} H = 0 \right\} \\
	&= \left\{ X \Omega + X_{\perp} Z \middle\vert \Omega \in \mathrm{Skew}(k), Z \in \mathbb{R}^{(m-k) \times k} \right\}
\end{aligned}\end{equation}},
$$
</p>
where \\(X_{\perp} \in \mathbb{R}^{m \times (m-k)}\\) is the orthogonal complement to \\(X\\) such that their column vectors form a basis of \\(\mathbb{R}^{m}\\) and \\(X^{\mathsf{T}} X_{\perp} = 0\\).

The orthogonal projection from \\(\mathbb{R}^{m \times k}\\) onto \\(T_{X} \mathrm{St}(m,k)\\) with respect to the Frobenius inner product \\(\langle X,Y\rangle = \mathrm{Tr}\\\{X^{\mathsf{T}} Y\\\}\\) is
<p>
$$
\boxed{\begin{equation}\begin{aligned}
	\Pi_X(H)
	&= H - X \cdot \mathrm{sym}\left(X^{\mathsf{T}} H\right) \\
	&= X \cdot \mathrm{skew}\left(X^{\mathsf{T}} H\right) + \left(\mathbb{I}_{m} - XX^{\mathsf{T}}\right) H
\end{aligned}\end{equation}}.
$$
</p>

<details>
	<summary>In the paper, a really nice derivation of the orthogonal projection is given, without invoking the regular value theorem. Hence I provide it here in details.</summary>

	Consider the metric of the Stiefel manifold being induced from the Frobenius inner product from the ambiant space. The normal space to the tangent space \(T_{X} \mathrm{St}(m,k)\) is denoted by \((T_{X} \mathrm{St}(m,k))^{\perp}\), consisting of matrices \(N\) that satisfies
	<p>
	$$
	\langle H, N\rangle = \mathrm{Tr}\left\{ H^{\mathsf{T}} N\right\} = 0
	$$
	</p>
	for all tangent vectors \(H \in T_{X} \mathrm{St}(m,k)\). From the second description of the tangent space, by assuming \(N = XS + X^{\perp} Z\) we can deduce that \(Z=0\) and \(S\) is symmetric. I.e., the normal space at \(X\) must be of the form
	$$
		\left\{ N \in \mathbb{R}^{m\times k} \middle\vert N = XS, S\text{ is symmetric} \right\}.
	$$
	Since we have the unique decomposition \(\mathbb{R}^{m \times k} = T_{X} \mathrm{St}(m,k) \oplus (T_{X} \mathrm{St}(m,k))^\perp\), there exists unique matrices \(\Omega\) (skew-symmetric), \(Z\), and \(S\) (symmetric) that satisfy
	$$
		H = X \Omega + X_{\perp} Z + XS.
	$$
	Henceforth, we obtain \(S = \mathrm{sym}(X^{\mathsf{T}} H)\), and
	$$
		\Pi_X(H) = H - XS = H - X \cdot \mathrm{sym}(X^{\mathsf{T}} H). \;\;\;\;\; \blacksquare
	$$
</details>

<br>

# Geodesic equation
This paper by Edelman et al. focuses at optimization over the orthogonally constrained sets of Stiefel manifolds or the Grassmanian manifolds. In manifold optimization, it is crucial to obtain the geodesics on the manifold so as to obtain second order or higher order derivatives. These are the main motivations of this work.

In differential geometry, the geodesics is defined with respect to a connection \\(\nabla\\) on the manifold. For our case, we consider the unique *Levi-Civita connection* induced by the Frobenius inner product.

Let \\(X(t)\\) be the geodesic curve on \\(\mathrm{St}(m,k)\\), its covariant derivative is zero with respect to itself. That it to say, it is of constant velocity on the manifold:
<p>
$$
\nabla_{\dot{X}} \dot{X} = 0.
$$
</p>
Since we consider the Stiefel manifold to be embedded within \\(\mathbb{R}^{m \times k}\\), we can equivalently say that the acceleration of the geodesic must be in the normal direction:
<p>
$$\begin{align*}
&\nabla_{\dot{X}} \dot{X} = \left(\ddot{X}\right)_{\mathrm{tangent}}= 0 \\
&\Rightarrow \ddot{X} \in \left(T_{X}\mathrm{St}(m,k)\right)^{\perp} \\
&\Rightarrow \ddot{X}(t) = X(t) S(t),
\end{align*}$$
</p>
where \\(S\\) is a symmetric matrix that depends on \\(t\\) as well. Furthermore, since \\(X^{\mathsf{T}}(t) X(t) = \mathbb{I}_m\\) for all \\(t\\),
<p>
$$
\begin{align*}
	&\frac{\mathrm{d}^2}{\mathrm{d}t^2} X^{\mathsf{T}} X = \ddot{X}^{\mathsf{T}} X + 2 \dot{X}^{\mathsf{T}} \dot{X} + X \ddot{X}^{\mathsf{T}} = 0 \\
	&\Rightarrow S X^{\mathsf{T}} X + 2 \dot{X}^{\mathsf{T}} \dot{X} + X^{\mathsf{T}} X S = 0 \\
	&\Rightarrow S = - \dot{X}^{\mathsf{T}} \dot{X}.
\end{align*}
$$
</p>
We have thus obtained the geodesic equation on the Stiefel manifold:
<p>
$$
\boxed{
	\ddot{X} + X \dot{X}^{\mathsf{T}} \dot{X} = 0
}.
$$
</p>
It should be noted that unlike in differential geometry, we like to express our equations under an extrinsic coordinate (\\(\mathbb{R}^{m \times k}\\)) when analyzing the geometry of a matrix manifold. The expressions beomce clean and coordinate-free.

# Solving the geodesic equation
Let us solve the geodesic equation! Instead of just giving you the answer and spoiling the fun, we will derive the solution from the geodesic equation step by step. Let us first state our initial conditions: we will be deriving a geodesic \\(\Gamma_{X,H}(t)\\) (abbreviated as \\(\Gamma\\))emanating from \\(X \in \mathrm{St}(m,k)\\) in the direction of \\(H \in T_X \mathrm{St}(m,k)\\). For an illustration, see the figure at the top of the post.

## The solution in the paper
Within the paper, they solved the geodesic equations by noticing the invariants of motion. The two invariants are
\\(\Gamma^{\mathsf{T}}(t) \Gamma(t) = \mathbb{I}\\) and \\(\Gamma^{\mathsf{T}}(t) \dot{\Gamma}(t) = A \\). The first one is trivial, and the second is obtained by fiddling around with the geodesic equation:
<p>
$$
\begin{align*}
	0 &= \ddot{\Gamma} + \Gamma \dot{\Gamma}^{\mathsf{T}} \dot{\Gamma} \\
	\Rightarrow 0 &= \Gamma^{\mathsf{T}} \ddot{\Gamma} + \dot{\Gamma}^{\mathsf{T}} \dot{\Gamma} \\
	&= \frac{\mathrm{d}}{\mathrm{d}t} \Gamma^{\mathsf{T}} \dot{\Gamma}.
\end{align*}
$$
</p>
Since \\(\dot{\Gamma}\\) is a tangent vector, we have \\(A\\) is skew-symmetric. Considering \\(t=0\\), we immediately see that \\(A = X^{\mathsf{T}} H\\). From these invariants, and from an observation[^Lippert] made by Ross Lippert, they reduced the second-order geodesic equation into a first-order differential equation that is easily solvable.

[^Lippert]: It is a stroke of genius in my opinion, as this definition of a state variable came out of the blue.

In detail: he considered a function \\(S(t) = \dot{\Gamma}^{\mathsf{T}} \dot{\Gamma}\\), satisfying
<p>
$$
\begin{align*}
	\dot{S}(t) &= \ddot{\Gamma}^{\mathsf{T}} \dot{\Gamma} + \dot{\Gamma}^{\mathsf{T}} \ddot{\Gamma} \\
	&= - \dot{\Gamma}^{\mathsf{T}} \dot{\Gamma} \Gamma^{\mathsf{T}} \dot{\Gamma} - \dot{\Gamma}^{\mathsf{T}} \Gamma \dot{\Gamma}^{\mathsf{T}} \dot{\Gamma} \\
	&= -S(t) A - A^{\mathsf{T}} S(t) \\
	&= AS - SA \\
	&= [A,S].
\end{align*}
$$
</p>
The solution to the differential equation above is
<p>
$$
	S(t) = \mathrm{e}^{tA} S_{0} \mathrm{e}^{-tA}
$$
</p>
with the initial state being \\(S_{0} = H^{\mathsf{T}} H\\).

Finally, consider the following equations:
<p>
$$
\begin{align*}
	\frac{\mathrm{d}}{\mathrm{d}t} \left[\begin{matrix}
		\Gamma \mathrm{e}^{tA} & \dot{\Gamma} \mathrm{e}^{tA}
	\end{matrix}\right] &= \left[\begin{matrix}
		\dot{\Gamma} \mathrm{e}^{tA} + \Gamma \mathrm{e}^{tA} A & -\Gamma \dot{\Gamma}^{\mathsf{T}} \dot{\Gamma} \mathrm{e}^{tA} + \dot{\Gamma} \mathrm{e}^{tA} A
	\end{matrix}\right] \\
	&= \left[\begin{matrix}
		\dot{\Gamma} \mathrm{e}^{tA} + \Gamma \mathrm{e}^{tA} A & -\Gamma \mathrm{e}^{tA} S_0  + \dot{\Gamma} \mathrm{e}^{tA} A
	\end{matrix}\right] \\
	&= \left[\begin{matrix}
		\Gamma & \dot{\Gamma}
	\end{matrix}\right] \left[\begin{matrix}
		A & -S_0 \\ \mathbb{I}_k & A
	\end{matrix}\right] \\
	\Rightarrow \left[\begin{matrix}
		\Gamma(t) \mathrm{e}^{tA} & \dot{\Gamma}(t) \mathrm{e}^{tA}
	\end{matrix}\right] &= \left[\begin{matrix}
		\Gamma(0) & \dot{\Gamma}(0)
	\end{matrix}\right] \cdot \exp\left(t \left[\begin{matrix}
		A & -S_0 \\ \mathbb{I}_k & A
	\end{matrix}\right]\right).
\end{align*}
$$
</p>

Thus, we have identified the geodesic to be
<p>
$$
\boxed{\begin{equation*}
	\Gamma_{X,H}(t) = \left[\begin{matrix}
		X & H
	\end{matrix}\right] \cdot \exp\left(t \left[\begin{matrix}
		X^{\mathsf{T}}H & -H^{\mathsf{T}}H \\
		\mathbb{I}_k & X^{\mathsf{T}}H
	\end{matrix}\right]\right) \cdot \mathbb{I}_{2k,k} \cdot \mathrm{e}^{-tX^{\mathsf{T}}H}
\end{equation*}},
$$
</p>
where \\(\mathbb{I}_{2k,k}\\) is the first \\(k\\) columns of \\(\mathbb{I} _{2k}\\).

## My solution
A different approach is given here. 

Inspired by how in control theory, the state space model allows us to transform high-order linear ODEs (ordinary differential equations) into a first order ODE, I introduce the state vector \\([\Gamma,\dot{\Gamma}]\\) and analyzed its derivative:
<p>
$$
\begin{align*}
	\frac{\mathrm{d}}{\mathrm{d}t} \left[\begin{matrix}
		\Gamma & \dot{\Gamma}
	\end{matrix}\right] &= \left[\begin{matrix}
		\dot{\Gamma} -\Gamma \dot{\Gamma}^{\mathsf{T}} \dot{\Gamma}
	\end{matrix}\right] = \left[\begin{matrix}
		\Gamma & \dot{\Gamma}
	\end{matrix}\right] \left[\begin{matrix}
		0 & -S(t) \\ \mathbb{I}_{k} & 0
	\end{matrix}\right], \\
	&= \lim_{\mathrm{d}t\rightarrow 0} \frac{1}{\mathrm{d}t} \left(\left[\begin{matrix}
		\Gamma(t+\mathrm{d}t) & \dot{\Gamma}(t+\mathrm{d}t)
	\end{matrix}\right] - \left[\begin{matrix}
		\Gamma(t) & \dot{\Gamma}(t)
	\end{matrix}\right]\right)\\	
	\Rightarrow \left[\begin{matrix}
		\Gamma & \dot{\Gamma}
	\end{matrix}\right] &= \left[\begin{matrix}
		\Gamma(0) & \dot{\Gamma}(0)
	\end{matrix}\right] \cdot \lim_{N\rightarrow\infty}\overrightarrow{\prod_{k=0}^{N-1}} \left(\mathbb{I} + \frac{t}{N} \left[\begin{matrix}
		0 & -S(tk/N) \\ \mathbb{I} & 0
	\end{matrix}\right]\right),
\end{align*}
$$
</p>
where \\(\overrightarrow{\prod}\\) is the product ordered from left to right and \\(\mathrm{d}t = t/N\\).

The infinite product can be further analysed. Note that since I am not a mathematician, I will describe what I did in each step in details, but not rigorously. First off, the infinite product reminds me of the (one of the) definition to the matrix exponential:
<p>
$$
\begin{align*}
	&\mathbb{I} + \frac{t}{N} \left[\begin{matrix}
		0 & -S(tk/N) \\ \mathbb{I} & 0
	\end{matrix}\right] \\
	&= \mathbb{I} + \frac{t}{N} \left[\begin{matrix}
		0 & -\mathrm{e}^{tkA/N} S_0 \mathrm{e}^{-tkA/N} \\ \mathbb{I} & 0
	\end{matrix}\right] \\
	&= \left[\begin{matrix}
		\mathrm{e}^{tkA/N} & 0 \\ 0 & \mathrm{e}^{tkA/N}
	\end{matrix}\right] \left(\mathbb{I} + \frac{t}{N}\left[\begin{matrix}
		0 & -S_0 \\ \mathbb{I} & 0
	\end{matrix}\right]\right) \left[\begin{matrix}
		\mathrm{e}^{-tkA/N} & 0 \\ 0 & \mathrm{e}^{-tkA/N}
	\end{matrix}\right].
\end{align*}
$$
</p>
The infinite product then becomes
<p>
$$
\begin{align*}
	&\lim_{N\rightarrow \infty} \left(\overrightarrow{\prod_{k=0}^{N-1}} \left(\mathbb{I} + \frac{t}{N} \left[\begin{matrix}
		0 & -S_0 \\ \mathbb{I} & 0
	\end{matrix}\right]\right) \left[\begin{matrix}
		\mathrm{e}^{tA/N} & 0 \\ 0 & \mathrm{e}^{-tA/N}
	\end{matrix}\right]\right) \left[\begin{matrix}
		\mathrm{e}^{-t(N-2)A/N} & 0 \\ 0 & \mathrm{e}^{-t(N-2)A/N}
	\end{matrix}\right] \\
	&= \lim_{N\rightarrow \infty} \left(\left(\mathbb{I} + \frac{t}{N}\left[\begin{matrix}
		0 & -S_0 \\ \mathbb{I} & 0
	\end{matrix}\right]\right) \left[\begin{matrix}
		\mathbb{I} + \frac{tA}{N} & 0 \\ 0 & \mathbb{I} + \frac{tA}{N}
	\end{matrix}\right]\right)^N \left[\begin{matrix}
		\mathrm{e}^{-tA} & 0 \\ 0 & \mathrm{e}^{-tA}
	\end{matrix}\right] \\
	&= \lim_{N\rightarrow \infty} \left(\mathbb{I} + \frac{t}{N} \left[\begin{matrix}
		A & -S_0 \\ \mathbb{I} & A
	\end{matrix}\right]\right)^N \left[\begin{matrix}
		\mathrm{e}^{-tA} & 0 \\ 0 & \mathrm{e}^{-tA}
	\end{matrix}\right] \\
	&= \exp\left(t \left[\begin{matrix}
		A & -S_0 \\
		\mathbb{I} & A
	\end{matrix}\right]\right) \cdot \left[\begin{matrix}
		\mathrm{e}^{-tA} & 0 \\ 0 & \mathrm{e}^{-tA}
	\end{matrix}\right].
\end{align*}
$$
</p>

The rest of the derivation is the same as in the paper and I'll not repeat them here. Hence, we obtain the same result as above.

To made the argument more rigorous, one should also check the convergence of the infinite product, and the approximations used do not affect the final product.

# Sanity check
As a small check for correctness, let us restrict our attention to the orthogonal matrices \\(\mathrm{O}(m) = \mathrm{St}(m,m)\\), with the tangent direction of the form \\(H = X\Omega\\), where \\(\Omega\\) is skew-symmetric. Then the geodesic will be
<p>
$$
\begin{align*}
	\Gamma_X(tH) &= \left[\begin{matrix}
		X & X\Omega \\
	\end{matrix}\right] \cdot \exp\left(t \left[\begin{matrix}
		\Omega & \Omega^2 \\ \mathbb{I}_k & \Omega
	\end{matrix}\right]\right) \left[\begin{matrix}
		\mathbb{I}_k \\ 0
	\end{matrix}\right] \cdot \exp\left(-\Omega t\right).
\end{align*}
$$
</p>
This can be greatly simplified. Notice that
<p>
$$
\begin{align*}
	\Gamma_X(tH) &= \left[\begin{matrix}
		X & X\Omega \\
	\end{matrix}\right] \left[\begin{matrix}
		\Omega & \Omega^2 \\ \mathbb{I}_k & \Omega
	\end{matrix}\right] \\
	&= \left[\begin{matrix}
		2X\Omega & 2X\Omega^2 \\
	\end{matrix}\right] \\
	&= \left[\begin{matrix}
		X & X\Omega \\
	\end{matrix}\right] \left[\begin{matrix}
		2\Omega & 0 \\ 0 & 2\Omega
	\end{matrix}\right].
\end{align*}
$$
</p>
Therefore, since
<p>
$$
\begin{align*}
	\left[\begin{matrix}
		2\Omega & 0 \\ 0 & 2\Omega
	\end{matrix}\right] \cdot \left[\begin{matrix}
		\Omega & \Omega^2 \\ \mathbb{I}_k & \Omega
	\end{matrix}\right] = \left[\begin{matrix}
		\Omega & \Omega^2 \\ \mathbb{I}_k & \Omega
	\end{matrix}\right] \cdot \left[\begin{matrix}
		2\Omega & 0 \\ 0 & 2\Omega
	\end{matrix}\right],
\end{align*}
$$
</p>
we have
<p>
$$
\begin{align*}
	\Gamma_X(tH) &= \left[\begin{matrix}
		X & X\Omega \\
	\end{matrix}\right] \cdot \exp\left(t \left[\begin{matrix}
		2\Omega & 0 \\ 0 & 2\Omega
	\end{matrix}\right]\right) \left[\begin{matrix}
		\mathbb{I}_k \\ 0
	\end{matrix}\right] \cdot \exp\left(-\Omega t\right) \\
	&= \left[\begin{matrix}
		X & X\Omega \\
	\end{matrix}\right] \cdot \left[\begin{matrix}
		\exp(2\Omega t) & 0 \\ 0 & \exp(2\Omega t)
	\end{matrix}\right] \left[\begin{matrix}
		\mathbb{I}_k \\ 0
	\end{matrix}\right] \cdot \exp\left(-\Omega t\right) \\
	&= X \exp(\Omega t) = X \exp(t X^{\mathsf{T}} H).
\end{align*}
$$
</p>
This is indeed the geodesic on the orthogonal matrices under the Frobenius norm.


---
A fun and quick journey right? But don't worry, this is not the end. There is more to this paper that I would love to dig deeper into. So stay tuned and wait for "Paper review: Geodesics of Stiefel Manifold (II)"!