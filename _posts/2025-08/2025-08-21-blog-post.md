---
title: 'First Blog! On Mirror Descent (I)'
date: 2025-08-21
permalink: /posts/2025/08/blog-post-1/
tags:
  - optimization
  - convex optimization
---

> This is a simple blog post on what mirror descent is. I learned this from a friend of mine, Zoya, who I met during my stay at ASL, EPFL in the summer of 2025.

# Descent methods revisited
A usual descent method is described by a recursion of the form
<p>
$$x_{t} = x_{t-1} - \eta g_{t},$$
</p>
where \\(\eta\\) is the step-size, and \\(g_{t}\\) is the descent direction. If the descent direction \\(g_{t}\\) is the gradient at \\(x_{t-1}\\), then the recursion is the gradient descent algorithm; if the direction is a subgradient, then the recursion is a subgradient descent; if the direction is the gradient multiplied by the inverse Hessian, then the recursion is the Newton's method.

Now, we can rewrite the recursion as an optimization problem of the form
<p>
$$\boxed{
    x_{t} = \arg\min_{x\in\mathbb{R}^n} \left\{ \langle g_{t},x\rangle + \frac{1}{2\eta} \left\| x - x_{t-1} \right\|_2^2 \right\}
}.$$
</p>
This can be readily checked via a simple differentiation. The minimization problem above can be seen as weighting between "being in the gradient direction" (the inner product term in the front) and "staying close to the previous point" (the norm term at the back).

What happens if we change the quadratic norm term at the back, also known as the *proximity term* with other forms of squared distances? This is the premise of mirror descent.

# What is mirror descent?
Let us consider a class of squared distances that can replace the  Euclidean squared distance we have above. These are called the Breman divergences.

## Bregman Divergence
Consider a strictly convex function \\(f:X\rightarrow\mathbb{R}\\), then the Bregman divergence associated with \\(f\\) for points \\(x\\) and \\(x' \in X\\) is defined to be
<p>
$$\boxed{ B_f(x, x') = f(x) - f(x') - \langle \nabla f(x') , x-x'\rangle} .$$
</p>

The Bregman divergence acts like a squared distance although being asymmetric. The discussions to the motivation of such a definition and what that previous sentence means are deferred to future posts on some basics of information geometry. So for the mean time, just bare with me here.

## Mirror descent
Now, replacing the proximity term in the first recursion with the Bregman divergence, we obtain
<p>
\begin{align*}
    \boxed{x_{t} = \arg\min_{x\in\mathbb{R}^n} \langle g_t,x\rangle + \frac{1}{\eta} B_f(x, x_{t-1})}.
\end{align*}
</p>

The choice of \\(f\\) is important as it changes the dynamic of the algorithm.

# Example of a mirror descent
## Euclidean distance
Consider the convex function as \\(f(x) = \frac{1}{2} || x ||_2^2\\), the scaled Euclidean \\(2\\)-norm. Then the Bregman divergence is
<p>
\begin{align*}
    B_f(x,x')
    &= \frac{1}{2} || x ||_2^2 - \frac{1}{2} \left\| x' \right\|_2^2 - \langle x', x - x' \rangle \\
    &= \frac{1}{2} \langle x + x', x - x'\rangle - \langle x', x - x'\rangle \\
    &= \frac{1}{2} \left\|x - x'\right\|^2.
\end{align*}
</p>
Under this choice of \\(f\\), we have obtained the usual descent method as introduced at the start of this post.

## KL divergence
Consider the convex function as the negative entropy:
<p>
$$
  f(x) = - H(x) = \sum_{k=1}^{n} x_k \log x_k
$$
</p>
defined over the probability simplex \\(X = \Delta([n])\\), i.e., \\(x\\) has non-negative components, \\(x_1 + \cdots + x_n = 1\\), and can be viewed as a distribution over a support of size \\(n\\). Here we choose the base to the logarithm as \\(e\\), but it does not really matter as it only contributes to a scaling in \\(\eta\\).

Next, let us compute the corresponding Bregman divergence:
<p>
$$\begin{aligned}
    B_f(x, x')
    &= \sum_{k=1}^{n} x_k \log x_k - \sum_{k=1}^{n} x'_k \log x'_k - \sum_{k=1}^{n} \left(x_k - x'_k\right) \left(1 + \log x'_k \right) \\
    &= \sum_{k=1}^{n} x_k \log \frac{x_k}{x'_k} \\
    &= D_{\mathrm{KL}}( x \| x' ).
\end{aligned}$$
</p>
This last function \\(D_{\mathrm{KL}}( x \\| x' )\\) is the Kullback-Leibler divergence between the discrete distributions \\(x\\) and \\(x'\\).

Plugging this into the mirror descent recursion, we see that
<p>
$$
    x_{t+1} = \arg\min_{x \in \Delta([n])} \left\{ \langle x,g_{t} \rangle + \frac{1}{\eta} D_{\mathrm{KL}}( x \| x_{t-1} ) \right\}.
$$
</p>
The minimizer is found by applying the method of Lagrange multipliers:
<p>
$$\begin{aligned}
    0 &= \frac{\mathrm{d}}{\mathrm{d} x_k} \langle x_{t}, g_{t} \rangle + \frac{1}{\eta} D_{\mathrm{KL}}( x_{t} \| x_{t-1} ) + \lambda \left(\sum_{k=1}^{n} x_{t,k} -1\right) \\
    &= g_{t,k} + \frac{1}{\eta} \left( 1 + \log x_{t,k} - \log x'_{t-1,k}\right) + \lambda \\
    \Rightarrow x_{t,k} &\propto \mathrm{e}^{-\eta g_{t,k}} x_{t-1,k}
\end{aligned}
$$
</p>
The Lagrange multplier \\(\lambda\\) is chosen such that \\(x\\) is a valid distribution. Henceforth, we arrive at
\\[
    x_{t,k} = \frac{\mathrm{e}^{-\eta g_{t,k}} x_{t-1,k}}{\sum_{\ell=1}^{n} \mathrm{e}^{-\eta g_{t,\ell}} x_{t-1,\ell}}.
\\]
This is, in fact, a *Bayesian update with exponential weighting*!
