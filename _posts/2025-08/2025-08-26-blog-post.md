---
title: 'BFGS Algorithm: a Faster Newton Descent'
date: 2025-08-26
permalink: /posts/2025/08/blog-post-6/
tags:
  - optimization
excerpt: ""
---

> This post discusses the BFGS algorithm, which is a famous quasi-Newton method for optimization that enjoys superlinear convergence but without the need of calculating the inverse to a Hessian.

You probably know all about the Newton's algorithm for achieving *quadratic convergence* near a local minima:
<p>
$$
	x_{t} = x_{t-1} - \underbrace{\left(H_f(x_{t-1})\right)^{-1} \nabla f(x_{t-1})}_{p_{t-1}},
$$
</p>
where \\(f\\) is the function to minimize, \\(\nabla f\\) is its gradient, \\(H_f\\) is its Hessian matrix, and \\(-p_{t-1}\\) is the descent direction.

The only downside to applying the Newton's method for optimization is that computing the inverse to the Hessian is too time-consuming. Furthermore, for some functions, it is not impossible to have any information on its second derivative. Therefore, *quasi-Newton methods* are proposed, which give *approximates* to the inverse of Hessian. Denote the approximate Hessian inverse as \\(C_{t-1} \approx (H_f(x_{t-1}))^{-1}\\), we are then concerned with the recursion of the form
<p>
$$
	x_{t} = x_{t-1} - \underbrace{C_{t-1} \nabla f(x_{t-1})}_{p_{t-1}},
$$
</p>

One such approximates is the [Broyden-Fletcher-Goldfarb-Shanno algorithm](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) (or simple, the BFGS algorithm). This post aims to give you my understanding of this algorithm from learning [online](https://youtu.be/yKf_ZoeoKmc?si=8iONcfuKv4YiLbVK). There definitely is better resources out there on the internet, but who cares.

The outline to a quasi-Newton algorithm is as follows.

<img src='/images/posts/2025-08-26-quasi-Newton.png'>

A reasonable initial condition will be to set \\(B_0 = \mathbb{I}\\), the identity matrix, as the first step will just become a gradient descent. Nothing wild.

# Approximating the Hessian
In optimization of a scalar function \\(f(x)\\), we one mentions having a \\(k\\)th-order oracle, it means that one has access to the values
<p>
$$
f(x), \nabla f(x), \nabla^2 f(x), \ldots, \nabla^k f(x)
$$
</p>
for all \\(x\\). We assume here that we only have a first-order oracle, hence we should deduce the Hessian based on the information from the gradients.

Denote our approximation of \\(H_f(x_{t-1})\\) as \\(B_{t-1}\\), the descent direction \\(-p_{t}\\) will be
<p>
$$\boxed{
	B_{t} p_{t} = \nabla f(x_{t})},
$$
</p>
and it should satisfy the *secant equation*:
<p>
$$\boxed{
	\nabla f(x_{t}) - \nabla f(x_{t-1}) = B_t \cdot (x_{t} - x_{t-1})}.
$$
</p>
From its name, we know that the above equation is an approximation to the tangent equation \\(\nabla f(x_{t-1}+\mathrm{d}x) - \nabla f(x_{t-1}) = B_t \cdot \mathrm{d}x\\).

<img src='/images/posts/2025-08-26-secant-equation.png'>

The final thing that needs to be discussed is, which is also the most important one, how do we update from \\(B_{t-1}\\) to \\(B_{t}\\)? Note that throughout the update, it is required that we have \\(B_{t} \succ 0\\) to ensure the stability of the algorithm.

## Rank-1 update
The rank-1 update is defined as
<p>
$$
	B_{t} = B_{t-1} + \alpha u u^{\mathsf{T}}
$$
</p>
for some scalar \\(\alpha\\) and vector \\(u\\). The problem, however, is that it does not preserve positive definiteness! If the approximate Hessian is no longer positive definite, we lose the *curvature*[^curvature] that we needed for stability and convergence.

[^curvature]: The curvature of a surface is related to its second derivative.

## Rank-2 update (BFGS)
If a rank-1 update does not work, why not try a rank-2 update? Consider the rank-2 update as
<p>
$$
	B_{t} = B_{t-1} + \alpha u u^{\mathsf{T}} + \beta v v^{\mathsf{T}}.
$$
</p>
Plugging in the above rank-2 update law into the secant equation: denote \\(s_{t-1} = x_{t} - x_{t-1}\\) and \\(y_{t-1} = \nabla f(x_{t}) - \nabla f(x_{t-1})\\), we have
<p>
$$
\begin{align*}
	y_{t-1} &= B_{t} s_{t-1} = B_{t-1} s_{t-1} + \alpha u u^{\mathsf{T}} s_{t-1} + \beta v v^{\mathsf{T}} s_{t-1} \\
	\Rightarrow s_{t-1}^{\mathsf{T}} y_{t-1} &= s_{t-1}^{\mathsf{T}} B_{t-1} s_{t-1} + \alpha (s_{t-1}^{\mathsf{T}} u)^2 + \beta (s_{t-1}^{\mathsf{T}} v)^2
\end{align*}
$$
</p>
A trivial choice for solution is to have \\(\alpha = 1/s_{t-1}^{\mathsf{T}} y_{t-1}\\), \\(u = y_{t-1}\\), \\(\beta = 1/s_{t-1}^{\mathsf{T}} B_{t-1} s_{t-1}\\), and \\(v = B_{t-1} s_{t-1}\\), resulting in
<p>
$$ \boxed{
B_{t} = B_{t-1} + \frac{y_{t-1} y_{t-1}^{\mathsf{T}}}{s_{t-1}^{\mathsf{T}} y_{t-1}} - \frac{B_{t-1} s_{t-1} s_{t-1}^{\mathsf{T}} B_{t-1}^{\mathsf{T}}}{s_{t-1}^{\mathsf{T}} B_{t-1} s_{t-1}} }.
$$
</p>

This is the BFGS algorithm for obtaining an approximate Hessian \\(B_{t}\\).

# Approximating the Hessian inverse
After we obtain the approximate Hessian, we need to invert it to actually obtain the descent direction \\(p_{t}\\). Luckily, the approximate form we have above admits a simple \\(O(n^2)\\) inverse formula, instead of the direct computation of inverse, requiring \\(O(n^3)\\) complexity.

Let us invoke the [Woodbury matrix inversion lemma](https://en.wikipedia.org/wiki/Woodbury_matrix_identity#Direct_proof)[^matinv]:

[^matinv]: I might write a future blog on this lemma. I have a favorite proof for it.

<p>

$$\boxed{
(A+UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1} V A^{-1}}.
$$
</p>

For the sake of clearity, we let \\(B_{t} = B^{+}\\), \\(B_{t-1} = B\\), \\(s_{t-1}=s\\), and \\(y_{t-1}=y\\). By applying this lemma and we obtain
<p>

$$
\begin{align*}
	B^+ &= B + \left[\begin{matrix}
		u & v
	\end{matrix}\right] \left[\begin{matrix}
		\alpha & 0 \\ 0 & \beta
	\end{matrix}\right] \left[\begin{matrix}
		u^{\mathsf{T}} \\ v^{\mathsf{T}}
	\end{matrix}\right] \\
	\Rightarrow (B^+)^{-1} &= B^{-1} - \left[\begin{matrix}
		B^{-1} y & s
	\end{matrix}\right] \left[\begin{matrix}
		s^{\mathsf{T}} y + y^{\mathsf{T}} B^{-1} y & y^{\mathsf{T}} s \\ y^{\mathsf{T}} s & 0
	\end{matrix}\right]^{-1} \left[\begin{matrix}
		y^{\mathsf{T}} B^{-1} \\ s^{\mathsf{T}}
	\end{matrix}\right] \\
	&= B^{-1} + \frac{1}{(y^{\mathsf{T}} s)^2} \left( - y^{\mathsf{T}} s \left(s y^{\mathsf{T}} B^{-1} + B^{-1} y s^{\mathsf{T}}\right) + \left(s^{\mathsf{T}} y + y^\mathsf{T} B^{-1} y\right) s s^{\mathsf{T}}\right).
\end{align*}
$$
</p>


Finally, reorganizing the terms above, we obtain the update law for the approximate inverse Hessian:
<p>

$$
\boxed{
	C_{t} = \left(\mathbb{I} - \frac{s_{t-1} y_{t-1}^{\mathsf{T}}}{y_{t-1}^{\mathsf{T}} s_{t-1}}\right) C_{t-1} \left(\mathbb{I} - \frac{y_{t-1} s_{t-1}^{\mathsf{T}}}{y_{t-1}^{\mathsf{T}} s_{t-1}}\right) + \frac{s_{t-1} s_{t-1}^{\mathsf{T}}}{y_{t-1}^{\mathsf{T}} s_{t-1}}.
}
$$
</p>

It should be noted that instead of finding a rank-2 update rule for approximating Hessian, then applying the matrix inversion lemma, we can instead work directly from the inverse Hessian, finding a rank-2 update rule for approximating the inverse Hessian. This latter approach is called the Davidon-Fletcher-Powell (DFP) update.

# Ensuring positive definiteness
The BFGS update we derived above depends heavily on \\(B_{t}\\) being positive definite, i.e.
<p>
$$
	s_{t-1}^{\mathsf{T}} B_{t} s_{t-1} = s_{t-1}^{\mathsf{T}} y_{t-1} > 0.
$$
</p>

The term \\(s_{t-1}^{\mathsf{T}} y_{t-1}\\) should be positive and bounded away from zero, this is the *curvature condition*.

The curvature condition expands out as
<p>

$$
	\langle x_{t} - x_{t-1}, \nabla f(x_{t}) - \nabla f(x_{t-1}) \rangle > 0.
$$
</p>
This is automatically satisfied for strictly convex functions! And we can, in this case, safely apply the BFGS algorithm.