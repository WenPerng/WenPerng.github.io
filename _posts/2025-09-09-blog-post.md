---
title: 'Stability of Newton-Schulz Iteration for Finding Matrix Inverses'
date: 2025-09-09
permalink: /posts/2025/09/blog-post-09/
tags:
  - optimization
  - linear algebra
excerpt: "This post discusses the stability of the order k Newton-Schulz iteration for finding matrix inverses."
---

The \\(k\\)th order Newton-Schulz iteration for finding inversion of the matrix \\(A\\), denoted by \\(X\\) is defined as
<p>

$$
X_{t+1} = X_{t} \sum_{n=0}^{k} (\mathbb{I} - AX_{t})^n.
$$
</p>

A fix point of the above iteration is \\(X_t = A^{-1}\\). Our task is to find the convergence rate of \\(X_t\\) to \\(A^{-1}\\) and the range of suitable initial conditions.

# Convergence analysis
Notice that as \\(k\rightarrow\infty\\), it is necessary that we have \\(||\mathbb{I} - AX_t|| < 1\\) for the infinite series to converge. Henceforth, for the \\(k\\)th order Newton-Schulz algorithm to converge, a sufficient condition will also be to require
<p>

$$
\|\mathbb{I} - AX_t\| < 1,
$$
</p>
where \\(||\cdot||\\) can be any matrix norm with \\(||\mathbb{I}|| = 1\\).

Then,
<p>

$$
\begin{align*}
    \|\mathbb{I} - AX_{t+1} \| &= \left\| \mathbb{I} - AX_{t} \sum_{n=0}^{k} (\mathbb{I} - AX_{t})^n\right\| \\
    &= \left\| (\mathbb{I} - AX_{t}) - AX_{t} \sum_{n=1}^{k} (\mathbb{I} - AX_{t})^n\right\| \\
    &= \left\| (\mathbb{I} - AX_{t}) \left(\mathbb{I} - AX_{t} \sum_{n=0}^{k-1} (\mathbb{I} - AX_{t})^n\right)\right\| \\
    &= \cdots \\
    &= \left\| (\mathbb{I} - AX_{t})^{k+1} \right\| \\
    &\le \| \mathbb{I} - AX_{t} \|^{k+1},
\end{align*}
$$
</p>
where the last step is by the sub-multiplicativity of matrix norms.

# Results
One can hence see that the convergence is geometric! With the \\(k\\)th order Newton-Schulz iteration following
<p>

$$
\boxed{
    \|\mathbb{I} - AX_{t+1} \| \le \|\mathbb{I} - AX_{t}\|^{k+1}.
}
$$
</p>
It's a really fast convergence. As long as the initial condition satisfies
<p>

$$
\boxed{
    \|\mathbb{I} - AX_0\| < 1.
}
$$
</p>