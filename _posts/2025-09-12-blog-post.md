---
title: 'Proximal Map'
date: 2025-09-12
permalink: /posts/2025/09/blog-post-12/
tags:
  - optimization
excerpt: ""
---

In the simplest Euclidean descent algorithms, I introduce how we can transform a descent optimization algorithm into a minimization problem
<p>

$$
    x_{t+1} = \arg\min_{x} \left\{\langle g,x\rangle + \frac{1}{2\eta} \|x-x_{t}\|^2\right\}.
$$
</p>
By replacing the squared distance with Bregman divergence, we obtain the mirror descent; replace it with norms, we obtained the steepest descent on normed spaces.

In this post, however, I discuss about what happens if we replace the inner product at the front with different things? It brings us to the optimization technique of proximal methods.

I have known the term "proximal maps" for a long time. I have met proximal maps before when dealing with Douglas-Rachford algorithms, also when I was learning about compressive sensing. Needless to say, the proximal map completes our basic toolbox of optimization after mirror descent. Reading through this [blog post](https://leloykun.github.io/ponder/steepest-descent-finsler/#3-general-solution-via-block-wise-primal-dual-hybrid-gradient-pdhg-algorithm), I came to see that it is about time I learn more thoroughly about proximal maps.

# Proximal method
This introduction to proximal methods takes main reference from [the book](https://web.stanford.edu/%7Eboyd/papers/pdf/prox_algs.pdf) by Parikh and Boyd.

Let \\(f\\) be a closed[^closed] convex function, then the proximal operator of \\(f\\) with step-size \\(\eta\\) is defined as

[^closed]: A *closed* convex function is one where its epigraph is closed.

<p>

$$
\mathrm{prox}_{\eta f}(v) = \arg\min_x \left\{f(x) + \frac{1}{2\eta}\|x - v\|_2^2\right\}.
$$
</p>
The norm \\(\\|\cdot\\|_2\\) is the usual Euclidean 2-norm. The minimizer is unique as the function inside is at least \\(1\\)-strongly convex.

## Examples

### \\(f\\) is differentiable
If \\(f\\) is differentiable, then we obtain
<p>

$$
\mathrm{prox}_{\eta f}(v) = v - \eta \nabla f(v),
$$
</p>
which is the usual gradient descent method! This can be readily checked by applying the first-order condition for extrema.

### Indicator function


# Interpreting proximal maps
The following result links proximal maps with fixed point theory: a point \\(x^\star\\) minimizes \\(f\\) if and only if
<p>

$$
\boxed{
    x^\star = \mathrm{prox}_f(x^\star)
}.
$$
</p>