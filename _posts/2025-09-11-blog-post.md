---
title: 'Steepest Descent on Normed Manifolds'
date: 2025-09-11
permalink: /posts/2025/09/blog-post-11/
tags:
  - manifold optimization
  - optimization
  - geometry
excerpt: ""
---

Continue from [last post](/posts/2025/09/blog-post-10/) where I discussed about Riemannian manifold optimizaiton, I extend my discussions on [steepest descent on normed vector spaces](/posts/2025/09/blog-post-07/) to normed manifolds.

I write this post after reading the works from [this site](https://docs.modula.systems/algorithms/manifold/). I do not like their formulation and introduction there. Hence, I decide to derive similar results from ground up in my own words.


From last post, I have established that to minimize a function defined on a manifold \\(f:\mathcal{M} \rightarrow \mathbb{R}\\), we apply the following two steps:
<p>

$$
{\begin{aligned}
    h &= \arg\min_{h\in T_{x_{t}}\mathcal{M}} \left\{ \frac{\partial f(x_{t})}{\partial x}[h] + \frac{1}{2} \langle h,h \rangle _{x_{t}}\right\}, \\
    x_{t+1} &= R_{x_t}(-\eta h).
\end{aligned}}
$$
</p>
The function \\(R\\) is a retraction chosen.

# Steepest descent on normed manifolds
The [post](https://leloykun.github.io/ponder/steepest-descent-non-riemannian/) by Franz Louis Cesista explained really well on this topic. My post is heavily influenced by it.

Consider on the tangent bundle \\(T\mathcal{M} = \cup_{x\in\mathcal{M}} T_x\mathcal{M}\\), instead of having an inner product \\(\langle\cdot,\cdot\rangle_x\\) associated to each tangent space \\(T_x\mathcal{M}\\), we have a norm \\(\|\|\cdot\|\|_x\\) on it. If the norm is induced by an inner product, we are doing Riemannian optimization; else, we are doing non-Riemannian optimization.

The descent direction \\(h\\) will then be
<p>

$$
\boxed{
    h = \arg\min_{h\in T_{x_{t}}\mathcal{M}} \left\{ \frac{\partial f(x_{t})}{\partial x}[h] + \frac{1}{2} \|h\|_x^2 \right\}.
}
$$
</p>
Solving this optimization then applying the retraction allows us to do steepest descent algorithms on normed manifolds.

