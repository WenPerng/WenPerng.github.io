---
title: 'First Blog! On Mirror Descent'
date: 2025-08-21
permalink: /posts/2025/08/blog-post-21/
tags:
  - optimization
---

This is a simple blog post on mirror descent.

I learned this from a friend of mine, Zoya, who I met during my stay at ASL, EPFL in the summer of 2025.

Descent methods revisited
======
A usual descent method is described by a recursion of the form
<p>
$$x_{t} = x_{t-1} - \eta g_{t},$$
</p>
where \\(\eta\\) is the step-size, and \\(g_{t}\\) is the descent direction. If the descent direction \\(g_{t}\\) is the gradient at \\(x_{t-1}\\), then the recursion is the gradient descent algorithm; if the direction is a subgradient, then the recursion is a subgradient descent; if the direction is the gradient multiplied by the inverse Hessian, then the recursion is the Newton's method.

Now, we can rewrite the recursion as an optimization problem of the form
<p>
$$ x_{t} = \arg\min_{x\in\mathbb{R}^n} \langle g_{t},x\rangle + \frac{1}{2\eta} \left\| x - x_{t-1} \right\|_2^2. $$
</p>
This can be readily checked via a simple differentiation.

What is mirror descent?
======

Example2
======

Example
------