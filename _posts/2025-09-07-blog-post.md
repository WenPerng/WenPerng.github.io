---
title: 'Steepest Descent in Normed Spaces'
date: 2025-09-07
permalink: /posts/2025/09/blog-post-07/
tags:
  - optimization
excerpt: ""
---

We have seen how we can perform steepest descent in inner product spaces by doing a gradient descent with the gradient defined as the *covector* \\(g\\). The next iterate for a vector (in the primal space) from \\(x_t\\) to  \\(x_{t+1}\\) is obtained from
<p>

$$
x_{t+1} = \arg\min_{x} \langle g, x\rangle + \frac{1}{2\eta} \| x - x_{t}\|^2,
$$
</p>
where \\(\|\cdot\|^2\\) is a *squared-distance* chosen,\\(\langle \cdot,\cdot\rangle\\) is an inner product that may or may not be associated with the square-distance, and \\(\eta\\) is the step-size (or in other words, it is the *inverse of sharpness*[^1]).

[^1]: The smaller the step-size, the sharper the optimization scheme becomes.

If the variable resides on the tangent space of a manifold, with the inner product and norm related to the metric of the manifold, then we obtain the usual Riemannian manifold optimization scheme. We will not go into manifold today, however.

Consider our variables living t=in the usual Euclidean space with the inner product being the usual Euclidean inner product, on the other hand: if \\(\|\cdot\|^2\|\\) is the usual square two-norm, then we obtain the usual gradient descent in Euclidean space; if it is replaced by the Bregman divergence, then we obtain the mirror descent. What if we simply replace the norm by a norm that need not be induced from an inner product?