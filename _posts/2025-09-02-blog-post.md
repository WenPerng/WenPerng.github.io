---
title: 'Complex Vector Differentiation'
date: 2025-09-02
permalink: /posts/2025/09/blog-post-02/
tags:
  - Linear Algebra
  - Optimization
excerpt: "This post continues on with the matrix differentiation I introduced in one of my [talks](/talks/2024-09-18-talk) in NTUEE into complex fields. We elaborate on how one can differentiate with respect to complex vectors."
---

This post continues on with the matrix differentiation I introduced in one of my [talks](/talks/2024-09-18-talk) in NTUEE into complex matrices. We will be considering scalar functions of complex variables: \\(g = g(z_1,\ldots,z_n,z_1^\*,\ldots,z_n^\*) \in \mathbb{C}\\), where \\(z_k\in\mathbb{C}\\). But first let us vectorize the complex variables: I use bold symbols to denote vectors and let \\(\boldsymbol{z} = [z_1,\ldots,z_n]^{\mathsf{T}}\\), then
<p>
$$
\boldsymbol{u} = \left[\begin{matrix}
  \boldsymbol{z} \\ \boldsymbol{z}^*
\end{matrix}\right] = \left[\begin{matrix}
  \boldsymbol{x} + \mathrm{i} \boldsymbol{y} \\ \boldsymbol{x} - \mathrm{i} \boldsymbol{y}
\end{matrix}\right] = \left[\begin{matrix}
	\mathbb{I}_n & \mathrm{i}\,\mathbb{I}_n \\ \mathbb{I}_n &-\mathrm{i}\,\mathbb{I}_n
\end{matrix}\right] \left[\begin{matrix}
  \boldsymbol{x} \\ \boldsymbol{y}
\end{matrix}\right] = D \boldsymbol{v},
$$
</p>
where \\(\boldsymbol{x}\\) and \\(\boldsymbol{y}\\) are real vectors of size \\(n\times1\\). The matrix \\(D\\) satisfies
<p>
$$DD^{\mathsf{H}} = 2\mathbb{I}_{2n}.$$
</p>
For a function \\(g(\boldsymbol{u})\\), we denote its realized version as \\(f\\):
<p>
$$g(\boldsymbol{u}) = f(\boldsymbol{v}).$$
</p>

# Complex Vector Differentiation
As we have already discussed the nuance to real vector differentiation, we can immediately borrow the results from there and extend to complex vector differentiation.

Again, the notation I used is partial differentiation \\(\partial / \partial \boldsymbol{u}\\) instead of the directional derivative \\(\mathrm{D}_{\boldsymbol{u}}\\).

First thing first, differentiation: since we require that
<p>
$$\begin{align*}
	\frac{\partial g}{\partial \boldsymbol{u}} [\mathrm{d}\boldsymbol{u}] &= \frac{\partial f}{\partial \boldsymbol{v}} [\mathrm{d}\boldsymbol{v}] \\
	&= \frac{\partial f}{\partial \boldsymbol{v}} [D^{-1}\mathrm{d}\boldsymbol{u}],
\end{align*}$$
</p>
we can rewrite in operator notation that
<p>
$$\boxed{
	\begin{aligned}
		\frac{\partial g}{\partial \boldsymbol{u}} &= \frac{\partial f}{\partial \boldsymbol{v}} \circ D^{-1} \\
		\frac{\partial f}{\partial \boldsymbol{v}} &= \frac{\partial g}{\partial \boldsymbol{u}} \circ D
	\end{aligned}
}.$$
</p>

Furthermore, by treating covectors as row vectors we can denote
<p>
$$
\frac{\partial g}{\partial \boldsymbol{u}} = \left[\begin{matrix}
	\frac{\partial g}{\partial \boldsymbol{z}} & \frac{\partial g}{\partial \boldsymbol{z}^*}
\end{matrix}\right], \frac{\partial f}{\partial \boldsymbol{v}} = \left[\begin{matrix}
	\frac{\partial f}{\partial \boldsymbol{x}} & \frac{\partial f}{\partial \boldsymbol{y}}
\end{matrix}\right].
$$
</p>
Since
<p>
$$
D^{-1} = \left[\begin{matrix}
	\frac{1}{2} \mathbb{I}_n & \frac{1}{2} \mathbb{I}_n \\ -\frac{\mathrm{i}}{2} \mathbb{I}_n & \frac{\mathrm{i}}{2} \mathbb{I}_n
\end{matrix}\right]
$$
</p>
we also obtain the famous *Wirtinger derivatives*:
<p>
$$\boxed{
\begin{align*}
	\frac{\partial g}{\partial \boldsymbol{z}} &= \frac{1}{2}\left(\frac{\partial f}{\partial \boldsymbol{x}} - \mathrm{i} \frac{\partial f}{\partial \boldsymbol{y}}\right) \\
	\frac{\partial g}{\partial \boldsymbol{z}^*} &= \frac{1}{2}\left(\frac{\partial f}{\partial \boldsymbol{x}} + \mathrm{i} \frac{\partial f}{\partial \boldsymbol{y}}\right).
\end{align*}
}$$
</p>

This can equally be checked by noticing that: for \\(\mathrm{d}\boldsymbol{u} = D \mathrm{d}\boldsymbol{v}\\)
<p>
$$
\begin{align*}
	\mathrm{d}g &= \frac{\partial g}{\partial \boldsymbol{u}}[\mathrm{d}\boldsymbol{u}] \\
	&= \frac{\partial g}{\partial \boldsymbol{z}}[\mathrm{d}\boldsymbol{z}] + \frac{\partial g}{\partial \boldsymbol{z}^*}[\mathrm{d}\boldsymbol{z}^*] \\
	&= \frac{1}{2}\left(\frac{\partial f}{\partial \boldsymbol{x}} - \mathrm{i} \frac{\partial f}{\partial \boldsymbol{y}}\right) [\mathrm{d}\boldsymbol{z}] + \frac{1}{2}\left(\frac{\partial f}{\partial \boldsymbol{x}} + \mathrm{i} \frac{\partial f}{\partial \boldsymbol{y}}\right) [\mathrm{d}\boldsymbol{z}^*] \\
	&= \frac{\partial f}{\partial \boldsymbol{x}} \left[\frac{\mathrm{d}\boldsymbol{z} + \mathrm{d}\boldsymbol{z}^*}{2}\right] + \frac{\partial f}{\partial \boldsymbol{y}} \left[\frac{\mathrm{d}\boldsymbol{z} - \mathrm{d}\boldsymbol{z}^*}{2 \mathrm{i}}\right] \\
	&= \frac{\partial f}{\partial \boldsymbol{x}} [\mathrm{d}\boldsymbol{x}] + \frac{\partial f}{\partial \boldsymbol{y}} [\mathrm{d}\boldsymbol{y}] \\
	&= \frac{\partial f}{\partial \boldsymbol{v}}[\mathrm{d}\boldsymbol{v}] = \mathrm{d}f.
\end{align*}
$$
</p>

## Real functions and gradient
If the function \\(g\in\mathbb{C}\\), and the induced inner product is the usual complex Euclidean inner product \\(\langle \boldsymbol{a},\boldsymbol{b}\rangle = \boldsymbol{a}^{\mathsf{H}} \boldsymbol{b}\\) (also known as the Hermitian inner product), then the gradient of \\(g\\) with respect to \\(u\\) will be the column vector
<p>
$$
	\nabla_{\boldsymbol{u}} g(\boldsymbol{u}) = \left(\frac{\partial g}{\partial \boldsymbol{u}}\right)^{\mathsf{H}} = D^{-\mathsf{H}} \left(\frac{\partial f}{\partial \boldsymbol{v}}\right)^{\mathsf{T}} = \frac{1}{2} D \nabla_{\boldsymbol{v}} f(\boldsymbol{v})
$$
</p>

Moreover, in optimization, we often have \\(g\in\mathbb{R}\\), then we have
<p>
$$
\left(\frac{\partial g}{\partial \boldsymbol{z}^*}\right)^* = \frac{\partial g}{\partial \boldsymbol{z}}.
$$
</p>
By using the real inner product on \\(\boldsymbol{z}\\) as \\(\langle \boldsymbol{a},\boldsymbol{b}\rangle = 2\mathrm{Re}\\\{\boldsymbol{a}^{\mathsf{H}} \boldsymbol{b}\\\}\\), we can restrict our discussion only to the \\(\boldsymbol{z}\\)-part and neglect \\(\boldsymbol{z}^\*\\). We have the gradient in this case as
<p>
$$
\boxed{\begin{aligned}
	&\mathrm{d}g = 2 \mathrm{Re}\left\{ \frac{\partial g}{\partial \boldsymbol{z}} \mathrm{d}\boldsymbol{z}\right\} = \left\langle \nabla_{\boldsymbol{z}} g, \mathrm{d}\boldsymbol{z}\right\rangle \\
	&\Rightarrow  \nabla_{\boldsymbol{z}}g = \left(\frac{\partial g}{\partial \boldsymbol{z}}\right)^{\mathsf{H}} = \left(\frac{\partial g}{\partial \boldsymbol{z}^*}\right)^{\mathsf{T}}
\end{aligned}}.
$$
</p>

A gradient descent algorithm with respect to the inner product \\(\langle \boldsymbol{a},\boldsymbol{b}\rangle = 2\mathrm{Re}\\\{\boldsymbol{a}^{\mathsf{H}} \boldsymbol{b}\\\}\\) can be easily implemented as \\(\boldsymbol{z}_{i+1} = \boldsymbol{z} _{i} + \mu \nabla _{\boldsymbol{u}}g(\boldsymbol{u} _{i})\\). The computation is also going to be faster because the dimension of \\(\boldsymbol{z}\\) is half as that of \\(\boldsymbol{u}\\).

---
One may question the inclusion of \\(\boldsymbol{z}^\*\\) from the initial start, after all, isn't it dependent on \\(\boldsymbol{z}\\) completely? Indeed, the two complex variables are complex conjugates to each other. The need for the inclusion of the conjugate part is so that we can use the usual *matrix-vector multiplication linear algebra* in the analysis. If the conjugate part is not included, one need to additionally introduce a *complex conjugation operator* that will be quite a nuisance to work with.

# Hessians
Here we consider the Hermitian inner product. Consider the second-order differential with \\(\mathrm{d}\boldsymbol{u}_1 = D\mathrm{d}\boldsymbol{u}_1\\) and \\(\mathrm{d}\boldsymbol{u}_2 = D\mathrm{d}\boldsymbol{u}_2\\), we have the bilinear operator
<p>
$$
\begin{align*}
	\frac{\partial^2 f}{\partial \boldsymbol{v}^2} [\mathrm{d} \boldsymbol{v}_1, \mathrm{d} \boldsymbol{v}_2] &= \mathrm{d} \boldsymbol{v}_1^{\mathsf{T}} \frac{\partial^2 f}{\partial \boldsymbol{v}^{\mathsf{T}} \partial \boldsymbol{v}} \mathrm{d} \boldsymbol{v}_2 \\
	&= \mathrm{d} \boldsymbol{v}_1^{\mathsf{T}} \frac{\partial}{\partial \boldsymbol{v}^{\mathsf{T}}} \left( \frac{\partial g}{\partial \boldsymbol{u}} D \mathrm{d} \boldsymbol{v}_2 \right) \\
	&= \mathrm{d} \boldsymbol{v}_1^{\mathsf{T}} D^{\mathsf{H}} \frac{\partial g}{\partial \boldsymbol{u}^{\mathsf{H}} \partial \boldsymbol{u}} D \mathrm{d} \boldsymbol{v}_2 \\
	&= \mathrm{d} \boldsymbol{u}_1^{\mathsf{H}} \frac{\partial g}{\partial \boldsymbol{u}^{\mathsf{H}} \partial \boldsymbol{u}} \mathrm{d} \boldsymbol{u}_2 = \frac{\partial^2 g}{\partial \boldsymbol{u}^2} [\mathrm{d} \boldsymbol{u}_1, \mathrm{d} \boldsymbol{u}_2].
\end{align*}
$$
</p>
we can thus define the real and complex Hessians matrices as
<p>
$$\boxed{
H_{\mathbb{R}} = \frac{\partial^2 f}{\partial \boldsymbol{v}^{\mathsf{T}} \partial \boldsymbol{v}}, H_{\mathbb{C}} = \frac{\partial^2 g}{\partial \boldsymbol{u}^{\mathsf{H}} \partial \boldsymbol{u}}
},$$
</p>
with \\(H_{\mathbb{R}}^{\mathsf{T}} = H_{\mathbb{R}}\\), \\(H_{\mathbb{C}}^{\mathsf{H}} = H_{\mathbb{C}}\\), and 
<p>
$$H_\mathrm{R} = D^{\mathsf{H}} H_\mathbb{C} D.$$
</p>

Note that we can also write out the submatrices explicitly as (again under the complex Euclidean inner product):
<p>
$$
H_\mathbb{R} = \nabla_{\boldsymbol{v}^{\mathsf{T}}} \nabla_{\boldsymbol{v}} f = \left[\begin{matrix}
	\partial_{\boldsymbol{x}^{\mathsf{T}}} \partial_{\boldsymbol{x}} f & \partial_{\boldsymbol{x}^{\mathsf{T}}} \partial_{\boldsymbol{y}} f \\
	\partial_{\boldsymbol{y}^{\mathsf{T}}} \partial_{\boldsymbol{x}} f & \partial_{\boldsymbol{y}^{\mathsf{T}}} \partial_{\boldsymbol{y}} f
\end{matrix}\right]
$$
</p>
and
<p>
$$
H_\mathbb{C} = \nabla_{\boldsymbol{u}^{\mathsf{T}}} \nabla_{\boldsymbol{u}} g = \left[\begin{matrix}
	\partial_{\boldsymbol{z}^{\mathsf{H}}} \partial_{\boldsymbol{z}} f & \partial_{\boldsymbol{z}^{\mathsf{H}}} \partial_{\boldsymbol{z}^*} f \\
	\partial_{\boldsymbol{z}^{\mathsf{T}}} \partial_{\boldsymbol{z}} f & \partial_{\boldsymbol{z}^{\mathsf{T}}} \partial_{\boldsymbol{z}^*} f
\end{matrix}\right].
$$
</p>

## Taylor expansion
The Taylor expansion of a complex function \\(g\\) can then be written generally as
<p>
$$\begin{align*}
	g(\boldsymbol{u} + \mathrm{d}\boldsymbol{u}) &= g(\boldsymbol{u}) + \frac{\partial g(\boldsymbol{u})}{\partial \boldsymbol{u}} [\mathrm{d} \boldsymbol{u}] + \frac{1}{2} \frac{\partial^2 g(\boldsymbol{u})}{\partial^2 \boldsymbol{u}} [\mathrm{d} \boldsymbol{u}, \mathrm{d} \boldsymbol{u}] + O(\|\mathrm{d} \boldsymbol{u}^3\|) \\
	&= g(\boldsymbol{u}) + \langle \nabla_{\boldsymbol{u}}g(\boldsymbol{u}), \mathrm{d}\boldsymbol{u}\rangle + \frac{1}{2} \langle H_\mathbb{C}(\boldsymbol{u}) [\mathrm{d} \boldsymbol{u}], \mathrm{d} \boldsymbol{u} \rangle + O(\|\mathrm{d} \boldsymbol{u}^3\|),
\end{align*}$$
</p>
where \\(H_\mathbb{C}(\boldsymbol{u})\\) is the complex Hessian *operator* evaluated at \\(\boldsymbol{u}\\). The exact representation to each component depends on the inner product used.

---
It should be noted that I truly enjoy the operator notation instead of the matrix-vector product notation, as it implicitly implies one using the complex Euclidean inner product.

By lifting oneself to the operator-level, the exact inner product used is no longer relevant. In optimization, however, we introduced back in an inner product to "evaluate" the performance of the descent method. The gradient used can then be chosen according to this inner product. But it need not be.

