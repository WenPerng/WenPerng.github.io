---
title: 'Riemannian Gradient Descent as a Minimization Task'
date: 2025-09-10
permalink: /posts/2025/09/blog-post-10/
tags:
  - manifold optimization
  - optimization
  - geometry
excerpt: "This post clarifies what I meant in a previous post on how gradient descent algorithms on Riemannian manifolds can also be recast into a minimization problem."
---

We have see how gradient descent in an Euclidean space \\(V\in\mathbb{R}^n\\): let \\(x_{t}\in V\\), \\(\eta\in\mathbb{R}\\), and \\(g \in V^\* \cong V\\),
<p>

$$
x_{t+1} = x_{t} - \eta g
$$
</p>
can be viewed equivalently as the minimization task of
<p>

$$
x_{t+1} = \arg\min_{x\in V} \left\{\langle g,x\rangle + \frac{1}{2\eta} \|x - x_{t}\|^2\right\}.
$$
</p>

Let us argue how this description can also be used in the case of Riemannian manifold optimization.

Note that I specify our task here as *Riemannian* as we are working with a manifold with a metric. This might no longer be the case if we work with steepest descent over a manifold equipped only with a *norm* and not an inner product (metric).

This post and a few upcoming ones are my thoughts after reading the works from [this site](https://docs.modula.systems/algorithms/manifold/).


# Riemannian manifold optimization recap
Given a manifold \\(\mathcal{M}\\) and loss function \\(f:\mathcal{M}\rightarrow\mathbb{R}\\). The gradient descent algorithm at \\(x_{t} \in \mathcal{M}\\) is defined as
<p>

$$
x_{t+1} = \mathrm{Exp}_{x_t}(-\eta g),
$$
</p>
where \\(\mathrm{Exp}_{x}(th)\\) is the geodesic emanating from \\(x\in\mathcal{M}\\) with velocity \\(h\in T_x\mathcal{M}\\) at time \\(t\in\mathbb{R}\\). The *Riemannian* gradient vector \\(g\in T_x\mathcal{M}\\) is the unique tangent vector that satisfies the relation
<p>

$$
\mathrm{D} f(x)[h] \equiv \frac{\partial f(x)}{\partial x} [h] = \langle g,h\rangle _x
$$
</p>
for all \\(h\in T_x\mathcal{M}\\), with \\(\langle\cdot,\cdot\rangle _x\\) being the metric endowed onto the manifold.

## Retraction
In application, computing the geodesics is often a daunting task. Instead, one might use a *retraction* map in place of the geodesic map.

By definition, a *retration* on a manifold \\(\mathcal{M}\\) is a smooth mapping \\(R\\) from the tangent bundle \\(T\mathcal{M} = \cup_{x\in\mathcal{M}} T_x\mathcal{M}\\) onto \\(\mathcal{M}\\) with the following properties. Let \\(R_x\\) denote the restriction of \\(R\\) to \\(T_x\mathcal{M}\\), then
- \\(R_x(0) = x\\), where \\(0\\) denotes the zero element on \\(T_x\mathcal{M}\\).
- With the canonical identification \\(T_0T_x\mathcal{M} \cong T_x\mathcal{M}\\), \\(R_x\\) satisfies \\(\mathrm{D} R_x(0) = \mathbb{I}_{T_x\mathcal{M}}\\), which is the identity map on \\(T_x\mathcal{M}\\).

Given a retraction \\(R\\), the descent
<p>

$$
x_{t+1} = R_{x_{t}}(-\eta g)
$$
</p>
is equally valid to be a gradient descent algorithm.

# Riemannian manifold optimization as a minimization task
## Euclidean case revisited
Let us separate the descent into two steps: (1) computing the descent direction, and (2) applying the descent.

For the Euclidean case, step (1) is to let the descent direction be \\(h\in T_{x_{t}}V \cong V\\) and let there be a dual vector \\(g\in V^\*\\), which is a function of \\(x_t\in V\\). The dual vector applies to a vector via the product \\(\langle\cdot,\cdot\rangle: V^\* \times V \rightarrow \mathbb{R}\\), not to be confused with the Euclidean inner product used above. Then, by noting that \\(x _{t+1} = x _{t} + h\\),
<p>

$$
h = \arg\min_{h\in V} \left\{\langle g,h\rangle + \frac{1}{2\eta} \|h\|^2\right\}.
$$
</p>
After obtaining the descent direction, the descent of step (2) is written as
<p>

$$
x_{t+1} = x_{t} - \eta g = x_{t} + h.
$$
</p>

## Riemannian case
As for the Riemannian manifold optimization, similar results can be established.

Step (1), again, is to find the descent direction \\(h \in T_x\mathcal{M}\\). Notice that by definition of the notations, we have \\(\partial f/\partial x \in T_x^\*\mathcal{M}\\), and for all \\(k \in T_x\mathcal{M}\\),
<p>

$$
\frac{\partial f(x)}{\partial x}[k] = \left\langle \frac{\partial f(x)}{\partial x} , k\right\rangle.
$$
</p>
We want \\(h\\) to be the scaled negative *Riemannian gradient* \\(-\eta\mathrm{grad}f(x)\\), i.e.,
<p>

$$
-\eta \frac{\partial f(x)}{\partial x}[k] = -\eta \langle \mathrm{grad}f(x),k\rangle _x = \langle h, k\rangle _x
$$
</p>
for all \\(k\in T_x\mathcal{M}\\), where \\(\langle\cdot,\cdot\rangle_x\\) is the inner product of the manifold at \\(x\\).

The result is that
<p>

$$
\boxed{
    h = \argmin_{h\in T_{x_t}\mathcal{M}} \left\{ \frac{\partial f(x_{t})}{\partial x}[h] + \frac{1}{2\eta} \langle h,h \rangle _{x_{t}}\right\}.
}
$$
</p>
The proof is as follows:
<p>

$$
\begin{align*}
    h &= \argmin_{h\in T_{x_t}\mathcal{M}} \left\{ \frac{\partial f(x_{t})}{\partial x}[h] + \frac{1}{2\eta} \langle h,h \rangle _{x_{t}}\right\} \\
    &= \argmin_{h\in T_{x_t}\mathcal{M}} \left\{ \langle \mathrm{grad}f(x_{t}), h \rangle _{x_t} + \frac{1}{2\eta} \langle h,h \rangle _{x_{t}}\right\} \\
    &\Leftrightarrow \frac{\partial}{\partial h} \langle \mathrm{grad}f(x_{t}), h \rangle _{x_t} + \frac{1}{2\eta} \langle h,h \rangle _{x_{t}} = 0 \\
    &\Leftrightarrow h = -\eta\cdot \mathrm{grad}f(x_{t}).
\end{align*}
$$
</p>
Thus it is shown.

Step (2) is to apply the retraction map as
<p>

$$
    x_{t+1} = R_{x_t}(h).
$$
</p>

A more standard form will be to replace the step-size by the sharpness parameter \\(\lambda = 1/\eta\\):
<p>

$$
\boxed{\begin{aligned}
    h_t &= \arg\min_{h\in T_{x_{t}}\mathcal{M}} \left\{ \frac{\partial f(x_{t})}{\partial x}[h] + \frac{\lambda}{2} \langle h,h \rangle _{x_{t}}\right\}, \\
    x_{t+1} &= R_{x_t}(h_t).
\end{aligned}}
$$
</p>

# Conclusion
The goal of this writeup is to establish the boxed equation above, showing that for Riemannian manifold optimization: The "product" between a dual vector and the descent direction is simply the pushforward differential map applied onto the descent direction, and it should be written that way to avoid misinterpretations. The squared distance term is replaced by the norm induced by the Riemannian metric on the manifold.

This also explained a misconception I had a few days ago. The term \\(\langle g,x\rangle\\) is NOT an inner product. I should disentangle the descent direction from the iteration point, as the former lives in the tangent space while the latter resides on the manifold itself. With them disentangled, one immediately identifies that \\(\langle g,h\rangle\\) is simply a pushforward map on the tangent space. By writing the product as a pushforward map, there will be no need in explicitly stating the dual vector.