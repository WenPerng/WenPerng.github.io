---
title: 'Riemannian Gradient Descent as a Minimization Task'
date: 2025-09-10
permalink: /posts/2025/09/blog-post-10/
tags:
  - manifold optimization
  - optimization
  - geometry
excerpt: ""
---

We have see how gradient descent in an Euclidean space \\(V\in\mathbb{R}^n\\): let \\(x_{t}\in V\\), \\(\eta\in\mathbb{R}\\), and \\(g \in V^\* \cong V\\),
<p>

$$
x_{t+1} = x_{t} - \eta g
$$
</p>
can be viewed equivalently as the minimization task of
<p>

$$
x_{t+1} = \arg\min_{x\in V} \left\{\langle g,x\rangle + \frac{1}{2\eta} \|x - x_{t}\|^2\right\}.
$$
</p>

Let us argue how this description can also be used in the case of Riemannian manifold optimization.

Note that I specify our task here as *Riemannian* as we are working with a manifold with a metric. This might no longer be the case if we work with steepest descent over a manifold equipped only with a *norm* and not an inner product (metric).

This post and a few upcoming ones are my thoughts after reading the works from [this site](https://docs.modula.systems/algorithms/manifold/).


# Riemannian manifold optimization recap
Given a manifold \\(\mathcal{M}\\) and loss function \\(f:\mathcal{M}\rightarrow\mathbb{R}\\). The gradient descent algorithm at \\(x_{t} \in \mathcal{M}\\) is defined as
<p>

$$
x_{t+1} = \mathrm{Exp}_{x_t}(-\eta g),
$$
</p>
where \\(\mathrm{Exp}_{x}(th)\\) is the geodesic emanating from \\(x\in\mathcal{M}\\) with velocity \\(h\in T_x\mathcal{M}\\) at time \\(t\in\mathbb{R}\\). The gradient vector \\(g\in T_x\mathcal{M}\\) is the unique tangent vector that satisfies the relation
<p>

$$
\mathrm{D} f(x)[h] \equiv \frac{\partial f(x)}{\partial x} [h] = \langle g,h\rangle
$$
</p>
for all \\(h\in T_x\mathcal{M}\\), with \\(\langle\cdot,\cdot\rangle\\) being the metric endowed onto the manifold.

## Retraction
In application, computing the geodesics is often a daunting task. Instead, one might use a *retraction* map in place of the geodesic map.

By definition, a *retration* on a manifold \\(\mathcal{M}\\) is a smooth mapping \\(R\\) from the tangent bundle \\(T\mathcal{M} = \cup_{x\in\mathcal{M}} T_x\mathcal{M}\\) onto \\(\mathcal{M}\\) with the following properties. Let \\(R_x\\) denote the restriction of \\(R\\) to \\(T_x\mathcal{M}\\), then
- \\(R_x(0) = x\\), where \\(0\\) denotes the zero element on \\(T_x\mathcal{M}\\).
- With the canonical identification \\(T_0T_x\mathcal{M} \cong T_x\mathcal{M}\\), \\(R_x\\) satisfies \\(\mathrm{D} R_x(0) = \mathbb{I}_{T_x\mathcal{M}}\\), which is the identity map on \\(T_x\mathcal{M}\\).

Given a retraction \\(R\\), the descent
<p>

$$
x_{t+1} = R_{x_{t}}(-\eta g)
$$
</p>
is equally valid to be a gradient descent algorithm.

# Riemannian manifold optimization as a minimization task


