---
title: 'Steepest Descent on Normed Spaces'
date: 2025-09-07
permalink: /posts/2025/09/blog-post-07/
tags:
  - optimization
categories:
  - Optimization
excerpt: ""
---

> Different from manifold optimization where we have an inner product induced norm, in this post, let us consider how we can perform steepest descent given an arbitrary norm.

We have seen how we can perform steepest descent in inner product spaces by doing a gradient descent with the gradient defined as the *dual vector* \\(g\\). The next iterate for a vector (in the primal space) from \\(x_t\\) to  \\(x_{t+1}\\) is obtained from
<p>

$$
x_{t+1} = \arg\min_{x} \langle g, x\rangle + \frac{1}{2\eta} \| x - x_{t}\|^2,
$$
</p>
where \\(\\|\cdot\\|^2\\) is a *squared-distance* chosen,\\(\langle \cdot,\cdot\rangle\\) is an inner product that may or may not be associated with the square-distance, and \\(\eta\\) is the step-size (or in other words, it is the *inverse of sharpness*[^1]).

[^1]: The smaller the step-size, the sharper the optimization scheme becomes.

If the variable resides on the tangent space of a manifold, with the inner product and norm related to the metric of the manifold, then we obtain the usual Riemannian manifold optimization scheme. We will not go into manifold today, however.

Consider our variables living in the usual Euclidean space with the inner product being the usual Euclidean inner product, on the other hand: if \\(\\|\cdot\\|^2\\) is the usual square two-norm, then we obtain the usual gradient descent in Euclidean space; if it is replaced by the Bregman divergence, then we obtain the mirror descent. What if we simply replace the norm by a norm that need not be induced from an inner product?

Before we move on, it should be noted that if the inner product is modified from the Euclidean inner product, then we are working with a different optimization method called the *proximal method*.

# Steepest descent under a norm
This is from the paper "[Modular Duality in Deep Learning](https://arxiv.org/pdf/2410.21265#:~:text=Modular%20dualization%20involves%20first%20assigning%20operator%20norms%20to,the%20weight%20space%20of%20the%20full%20neural%20architecture.)", discussing how we can apply steepest descent method under a norm. Consider the iteration \\(x_{t+1} = x_t + \Delta x\\), what is the value of \\(\Delta x\\) given a *sharpness* \\(\lambda\\) and gradient \\(g\\)?

From the minimization formulation above, we see that a reasonable choice will be
<p>

$$
\Delta x = \arg\min_{\Delta x} \left\{ \langle g,\Delta x\rangle + \frac{\lambda}{2} \|\Delta x\|^2 \right\}.
$$
</p>

Why the use of a different norm? As I talked about in another [post](https://wenperng.github.io/posts/2025/08/blog-post-1/) before, the use of a non-Euclidean norm allows us to give different *weighting* to different coordinate directions of the change in value \\(\Delta x\\) with respect to the gradient \\(g\\).

The numerical value of \\(g\\) might be large for a certain direction. But by using a different norm, the change \\(\Delta x\\) will favor a different direction.

## Solving the minimization problem
The solution to the minimization program above is
<p>

$$
\arg\min_{\Delta x\in V} \left\{ \langle g,\Delta x\rangle + \frac{\lambda}{2} \|\Delta x\|^2 \right\} = -\frac{\|g\|_*}{\lambda} \cdot \mathrm{dualize}_{\|\cdot\|}(g),
$$
</p>
where \\(\\|\cdot\\|_\*:V^\*\rightarrow\mathbb{R}\\) is the dual norm defined as
<p>

$$
\|g\|_* = \max_{x\in V,\|x||=1} \langle g,x\rangle = \max_{x\in V,\|x||=1} g^\mathsf{T} x
$$
</p>
and 
<p>

$$
\mathrm{dualize}_{\|\cdot\|}(g) = \arg\max_{x\in V,\|x||=1} \langle g,x\rangle
$$
</p>
is the duality map mapping a dual vector to a vector.

Let us prove this from scratch, the proof is from the work "[Old Optimizer, New Norm: An Anthology](https://arxiv.org/abs/2409.20325v1)": let the change \\(\Delta x = ct\\) where \\(c \ge 0\\) is a constant and \\(t\\) is a vector of length 1 in \\(\\|\cdot\\|\\). Then,
<p>

$$
\begin{align*}
    \arg\min_{\Delta x\in V} \left\{ \langle g,\Delta x\rangle + \frac{\lambda}{2} \|\Delta x\|^2 \right\} &= \arg\min_{\Delta x\in V} \left\{ c \langle g,t\rangle + \frac{\lambda}{2} c^2 \right\} \\
    &= \arg \min_{c\ge 0} \left\{ c \min_{t\in V, \|t\|=1} \langle g,t\rangle + \frac{\lambda}{2} c^2 \right\} \\
    &= \arg\min_{c\ge 0} \left\{-c \|g\|_* + \frac{\lambda}{2}c^2\right\} \cdot \mathrm{dualize}_{\|\cdot\|}(g) \\
    &= -\frac{\|g\|_*}{\lambda} \cdot \mathrm{dualize}_{\|\cdot\|}(g).\;\;\;\;\;\blacksquare
\end{align*}
$$
</p>
Thus we have shown.

# Origin of a Normed Space
This type of question naturally arises when optimizing a neural network. Consider an input space \\(x\in X\\) with norm \\(\\|\cdot\\| _\alpha\\) and an output space \\(y\in V\\) with norm \\(\\|\cdot\\| _\beta\\). The two spaces are related by a linear transform \\(y = Mx\\). Then there is a naturally induced norm on the weights \\(M\\) in the weight space defined as
<p>

$$
\|M\|_{\alpha\rightarrow\beta} = \max_{x} \frac{\|Mx\|_\beta}{\|x\|_\alpha}.
$$
</p>

In fact, by adopting the viewpoint of steepest descent on a normed space, one can interpret the Adam optimizer differently from that of the convex optimization theory. That will be a thing for me to learn in the future. These can be found from the paper "[Old Optimizer, New Norm: An Anthology](https://arxiv.org/abs/2409.20325v1)". Definitely worth a read.