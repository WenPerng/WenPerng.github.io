---
title: 'Forward and Backward KL: Interpreting via Information Geometry'
date: 2025-08-30
permalink: /posts/2025/08/blog-post-10/
tags:
  - information geometry
  - information theory
  - geometry
excerpt: "Following from the last post where we introduced the e- and m-properties of information geometry, I will give an interpretion of the forward and backward KL divergence in this post."
---

Following from the last post where we introduced the \\(e\\)- and \\(m\\)-properties of information geometry, I will give an interpretion of the forward and backward KL divergence in this post.

But first off, for those who are unsure of the details, let me introduce first what the forward and backward KL divergences are.

# Forward and backward KL divergences
If we want to match a true underlying distribution \\(P\\) by an approximate distribution \\(Q\\), we can minimize the "distance" between them. Suppose we restrict \\(Q\\) to be in a subset \\(\mathcal{S}\\), we can define it to be
<p>
$$
    Q = \arg\min_{Q\in\mathcal{S}} D_{\mathrm{KL}}(P\|Q).
$$
</p>
This is called the *forward* KL divergence, and it is used to perform *moment matching* or *mean seeking*.

On the other hand, for
<p>
$$
    Q = \arg\min_{Q\in\mathcal{S}} D_{\mathrm{KL}}(Q\|P),
$$
</p>
this is called the *backward* KL divergence, and it is used to perform *mode finding*.

# Interpretation using information geometry

