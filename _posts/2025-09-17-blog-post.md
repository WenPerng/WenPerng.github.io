---
title: 'Robbins-Monro Step-Size Condition'
date: 2025-09-17
permalink: /posts/2025/09/blog-post-17/
tags:
  - optimization
excerpt: "This post discusses a result on the Robbins-Monro step-size condition which guarantees convergence under a noisy gradient descent."
---

In dealing with iterations of the form
<p>

$$
x_{t+1} = x_{t} - \mu \boldsymbol{g}_t,
$$
</p>
the iteration might not converge given a fixed step-size \\(\mu\\) and a gradient with noise denoted by \\(\boldsymbol{g} _t\\).

The Robbins-Monro condition on an iteration-dependent scheme ensures the convergence of the iteration above. Let the step-size on iteration \\(t\\) be \\(\mu_t\\), then the condition reads
<p>

$$
\boxed{
\begin{aligned}
    \sum_{t=1}^{\infty} \mu_t =\infty, \\
    \sum_{t=1}^{\infty} \mu_t^2 < \infty.
\end{aligned}
}
$$
</p>

# Result
For this post specifically, it is to proof Lemma F.5 in the book "[Adaptation, Learning, and Optimization over Networks](https://asl.epfl.ch/asl-book/adaptation-learning-and-optimization-over-networks/)" by Ali H. Sayed. This lemma can be used to show the convergence of the noisy gradient descent recursion above under Robbins-Monro condition by a few change of variables. The lemma is stated as:

---

Let \\(x_t \ge 0\\) denote a scalar deterministic sequence that satisfies the inequality recursion:
<p>

$$
x_{t+1} \le \left(1 - a_t\right) x_t + b_t\;\;\;\;\;(t\ge 0).
$$
</p>

When the scalar sequences \\(\\\{a _t, b _t\\\}\\) satisfy the four conditions:
<p>

$$
\begin{aligned}
    &0 \le a_t < 1, &&b_t \ge 0, \\
    &\sum_{t=1}^{\infty} a_t = \infty, &&\lim_{t\rightarrow\infty} \frac{b_t}{a_t} = 0,
\end{aligned}
$$
</p>
it holds that \\(\lim _{t\rightarrow\infty} x _t = 0\\).

When the scalar sequences \\(\\\{a _t, b _t\\\}\\) are of the form
<p>

$$
\begin{aligned}
    &a_t = \frac{c}{t}, &&b_t = \frac{d}{t^{p+1}}
\end{aligned}
$$
</p>
for \\(c, d, p > 0\\), it holds that for large \\(t\\), the sequence \\(x _t\\) converges to \\(0\\) at one of the following rates:
<p>

$$
\begin{cases}
    x_t \le \left(\frac{d}{c-p}\right) \frac{1}{t^p} + o\left(\frac{1}{t^p}\right), & c>p; \\
    x_t = O\left(\frac{\log t}{t^p}\right), & c=p; \\
    x_t = O\left(\frac{1}{t^c}\right), & c<p.
\end{cases}
$$
</p>

---

This result is an excerpt from "Introduction to Optimization" by B. Poljak. This post is to adapt the proof there to a more modern version.



# Proof