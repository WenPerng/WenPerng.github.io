---
title: 'BFGS Algorithm: a Faster Newton Descent'
date: 2025-08-26
permalink: /posts/2025/08/blog-post-6/
tags:
  - optimization
excerpt: "This post discusses the BFGS algorithm, which is a famous quasi-Newton method for optimization that enjoys superlinear convergence but without the need of calculating the inverse to a Hessian."
---

You probabily know all about the Newton's algorithm for achieving *quadratic convergence* near a local minima:
<p>

$$
	x_{t} = x_{t-1} - \left(H_f(x_{t-1})\right)^{-1} \nabla f(x_{t-1}),
$$
</p>
where \\(f\\) is the function to minimize, \\(\nabla f\\) is its gradient, and \\(H_f\\) is its Hessian matrix.

The only downside to applying the Newton's method for optimization is that computing the inverse to the Hessian is too time-consuming. Furthermore, for some functions, it is not impossible to have any information on its second derivative. Therefore, *quasi-Newton methods* are proposed, which give *approximates* to the inverse of Hessian.

One such approximates is the [Broyden-Fletcher-Goldfarb-Shanno algorithm](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) (or simple, the BFGS algorithm). This post aims to give you my understanding of this algorithm. There definitely is better resources out there on the internet, but who cares.

# Approximating the Hessian Inverse