---
title: 'The \\(e\\) and \\(m\\) in Information Geometry'
date: 2025-08-29
permalink: /posts/2025/08/blog-post-9/
tags:
  - information geometry
  - information theory
  - geometry
excerpt: "Besides the exponential family, another type of important family of distributions is the mixture families. This post gives some illustration to the e- and m-properties in information geometry, completing the basic picture of information geometry."
---

Besides the exponential family that we talked about [last time](/posts/2025/08/blog-post-5/), the *mixture family* that I am about to introduce you to right now is also fairly important.

The exponential family together with the mixture family completes the view on the *flat structures* of some information manifolds.

Stay till the end where I give an interpretation of the forward and backward KL divergences used in machine learning.

# Mixture family
Given a set of distributions \\(\\\{q_k(x)\\\}_{k=0}^{n}\\), an element in the mixture family is given by
<p>
$$
p(x\vert\eta) = \sum_{k=0}^{n} \eta_k q_k(x),
$$
</p>
where the coordinates \\(\eta_k>0\\) satisfy
<p>
$$
\sum_{k=0}^{n} \eta_k = 1.
$$
</p>
This is obviously an \\(n\\)-dimensional manifold.

## Examples of mixture family
The simplest example is the discrete distributions \\(p\in\Delta([n+1])\\), where \\(q_k(x) = \delta(x-k)\\) and \\(\eta_k = p_k\\).

## Connection with exponential family
Remember how that when we mentioned the discrete distributions as an example to the exponential family a while back, we derivede the dual coordinates identically the same as that above!

This gives us an inspiration on how we can continue with our study even for those distributions in the mixture family that are NOT in the exponential family.

Let us first recall again a distribution in the exponential family
<p>
$$
p(x\vert\theta) \, \mathrm{d}\mu(x) = \exp(\theta^{\mathsf{T}}x +k(x) - \psi(\theta))\, \mathrm{d}x
$$
</p>
has a natural convex function \\(\psi\\) associated with it. I have actually found a name for this function, most literature either call it the *free energy* of the statistical model, or the *cumulant generating function* (used by Amari). Now, what we did not discuss last time is its convex conjugate (denoted by \\(\varphi\\)): we have, for \\(\eta = \nabla\psi(\theta) = \mathbb{E}[x]\\),


Hence, the convex conjugate is the *negative entropy*:
<p>
$$
\begin{align*}
	\varphi(\eta) &= \theta^{\mathsf{T}} \eta - \psi(\theta) \\
	&= \mathbb{E}\left[\theta^{\mathsf{T}} x - \psi(\theta)\right] \\
	&= \mathbb{E}\left[\log p(x\vert\theta)\right].
\end{align*}
$$
</p>
The negative entropy is also a convex function. Those with a careful mind should go back to the other post and check that the \\(\varphi\\) we derived for Gaussian distributions is exactly its negative entropy.

Henceforth, whether it be an exponential family or not, we associate with each mixture family the negative entropy as a convex function that can be used to transform between the dual and the primal coordinates.

## Primal and dual coordinates for exponential family
To restate what we have observed above: for a mixture family, not necessarily to be in the exponential family, we have associated with it a convex function (which is identical to its negative entropy):
<p>
$$
\boxed{
	\varphi(\eta) = \int p(x\vert\theta) \log p(x\vert\theta) \,\mathrm{d}\mu(x)
}.
$$
</p>
With this convex function, we can transform from the \\(\eta\\) coordinates (the dual coordinates) to the \\(\theta\\) coodinates (the primal coordinates) by the transformation
<p>
$$\boxed{\theta = \nabla\varphi(\eta)}.$$
</p>
And obviously, the Bregman divergence given by \\(\varphi\\) between two distributions \\(P\\) and \\(Q\\) in the mixture family is their KL divergence:
<p>
$$\boxed{
B_{\varphi}(\eta(P),\eta(Q)) = D_{\mathrm{KL}}(P\|Q)
}.
$$
</p>

# Flatness of information manifold
In information geometry, the interplay between the primal and dual is really important. As they strongly associate themselves with the exponential family and the mixture family, one often see primal or dual terms written with \\(e\\) or \\(m\\) in front, respectively. These two letters obviously stands for the exponential and the mixture.

## \\(e\\)- and \\(m\\)-geodesics
For an exponential family, we can define an \\(e\\)-geodesic (also called a *primal* geodesic) on the primal coordinates \\(\theta\\) between two distributions \\(p(x\vert\theta_1)\\) and \\(p(x\vert\theta_2)\\) as
<p>
$$\boxed{
	\theta(t) = (1-t)\theta_1 + t\theta_2
}.$$
</p>
This is another member in the exponential family:
<p>
$$
p(x\vert\theta(t)) = \exp\left(\theta(t)^{\mathsf{T}}x - \psi(\theta(t))\right).
$$
</p>
By taking the logarithm, one can also see that the \\(e\\)-geodesic is a linear interpolation (with normalization) in logarithmic scale.

In terms of the dual coordinates for the exponential family, an \\(m\\)-geodesic (also called a *dual* geodesic) between \\(p(x\vert\eta_1)\\) and \\(p(x\vert\eta)\\)[^abuse] is
<p>
$$\boxed{
	\eta(t) = (1-t)\eta_1 + t\eta_2
}.$$
</p>
On the \\(m\\)-geodesic, the expectation of \\(x\\) is linearly interpolated.

[^abuse]: Note the abuse of notation right here.

On the other hand, for the mixture family, the \\(m\\)-geodesic is simply a linear interpolation of the distributions themselves. But bare in mind that, in general, the linear interpolation of distributions in an exponential family might not be in the exponential family.

## \\(e\\)- and \\(m\\)-flatness
A family of probability distributions, whether it is an exponential family, mixture family, or some other exotic distribution, is called \\(e\\)-flat if it defined by a linear constraint in \\(\theta\\); it is called \\(m\\)-flat if it is defined by a linear constraint in \\(\eta\\).